---
title: Final Model Fit
author: 'Ouafaa Hmaddi, Kivalina Grove, and Alexis Adams-Clark'
date: '2020-12-05'
slug: final-model-fit
categories: []
tags: []
---

This post will discuss the selection of our final model fit and the evidence that led us to this selection. We will first compare and contrast the three decision tree-based model fits. 

Model selection is not a straight forward process because, as you have seen in our model posts, we had to make a myriad of decisions when designing and configuring a prediction model. For example, we needed to make decisions about hyperparameter tuning and the structure of the recipe among many other decisions. In almost all of these decisions, we used heuristics based on our knowledge and benchmark, tested our decisions on small fractions of data, evaluated the final metrics focusing on minimizing the RMSE, and finally applied our model to the unseen test data (validation set).  

Therefore, in conducting this type of model building, it is important to have a robust test harness that allows you to estimate the performance of a given model configuration on unseen data, and reliably compare the performance to other model configurations. In this post, we will attempt to walk you through our comparison steps. 

First, we collected metrics of the 3 models. The metrics below show the final results of the out-of-sample performance for each of our three models:

-  *Bagged tree metrics* tuned on 1% of the data, final prediction on full training dataset:
      - rmse = 113.8576209 
      - rsq = 0.2422997
      - computation time (on personal computer) was about 21600 seconds to tune and fit. Of note, the tuning was first tested based on a fraction of data and the resulting estimates of the hyperparameters were used for the final prediction. 

-  *Random forest metrics* based on tuned model applied to the full training dataset: 
      - rmse = 85.8138947197454
      - rsq = 0.452623773165699
      - computation time (on Talapas, the University of Oregon virtual super computer) was exactly 22911.134 seconds to tune and fit.

-  *Boosted tree metrics* based on the tuned model applied to full training dataset: 
      - rmse = 85.2073125
      - rsq = 0.4645603
      - computation time (on Talapas) was over 25,200 seconds to tune and fit.  

While these metrics are not 100% standardized for comparison given that some models were run on a super computer while others were run on a personal computer and some are based on metrics from tuning a fraction of the data while others are based on training based on the full training data, they can still give us an idea of their computational efficiency. Overall, the estimates of metrics you get on a fraction of data are valid to give you an idea of your model performance. Furthermore, the differences in terms of timing between a super computer and a personal computer should be taken into account but since we will be mainly comparing the random forest to the boosted tree given how close their metrics are, this should not be an issue because they were both ran on Talapas, the University of Oregon super computer.

Our findings of rmse and rsq are aligned with our general knowledge. That is, boosting usually outperforms random forest and bagging, but random forest is easier to implement and has much better computational efficiency. In bagging, the number of predictors is already set, and we only tuned two hyperparamerers, the number of decision `trees` to be created, and the minimum number of data points present in a terminal node, `min_n`. In random forest, we can tune many hyperparameters, including those we examined, namely the number of predictors sampled at each split `mtry`, the number of `trees` and `min_n`. However, in boosting, more tuning parameters are required, namely based on what we examined, tree depth and learning rate.

Now that we agree that boosted trees performed better, which model do we choose, the tuned model or the untuned model? One important aspect to discuss here is that tuning the model took us at least seven hours using Talapas, and even then we ended with a worse performance $(rmse = 85.2073125)$ compared to our initial untuned model $(rmse = 85.0943206)$. This was pretty disappointing, as we expected a much better performance as a result of this costly tuning process. We think that what might have happened here is that tuning ended up picking up some noise in the training data in its learning process, which basically means that by tuning our model, we ended up over fitting it, so that while tuning improved accuracy for the data set on which it was trained, it decreased out-of-sample accuracy for the validation set. 

Therefore, our untuned boosted tree model won the race based on its performance metric $(rmse = 85.0943206)$. Additionally, although it required more computational resources than the random forest model, it required fewer resources than the tuned boosted tree model. That said, it is important to note that the differences in performance between random forest and boosted tree models is not that large, so if computational efficiency is an important issue, we would also expect our random forest model to perform well, and be much more computationally efficient following the tuning process.
 
As we described in our boosted tree post, we will submit our final model predictions to Kaggle so that a quantitative indicator of prediction accuracy can be provided.

NOTE: We all computed final predictions based on the `test.csv` dataset. Because of the computational intensity of several of our datasets, in order to ease submission once we had selected the best model based on rmse from our validation set, these predictions were generated before comparison of our models. Our testing data set was not used for training in any way, and these prediction files for the test data set were only used to submit to the Kaggle competition. Only the predictions from our our best model, the untuned boosted tree model, were submitted to Kaggle, where they scored 83.99265 on the [public leaderboard](https://www.kaggle.com/c/edld-654-fall-2020/leaderboard) using 30% of the test data.  