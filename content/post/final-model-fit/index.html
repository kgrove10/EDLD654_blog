---
title: Final Model Fit
author: ''
date: '2020-12-05'
slug: final-model-fit
categories: []
tags: []
---

<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<p>This post will discuss the selection of our final model fit and the evidence that led us to this selection. We will first compare and contrast the three decision tree-based model fits.</p>
<p>Model selection is not a straight forward process because as you have seen in our previous posts, we had to make a myriad of decisions when designing and configuring a prediction model. For example, We needed to make decisions about hyperparameter tuning and the structure of the recipe among many other decisions. In almost all of these decisions, we used heuristics based on our knowledge and benchmark, tested our decisions on small fractions of data, evaluated the final metrics focusing on minimizing the RMSE, to finally apply our model to the unseen test data.</p>
<p>Therefore, in conducting this type of model building, it is important to have a robust test harness that allows you to estimate the performance of a given model configuration on unseen data, and reliably compare the performance to other model configurations. In this post, we will attempt walk you through our comparison steps.</p>
<p>First, we collected metrics of the 3 models. The metrics below show the final results of the out-of-sample performance for each of our three models:</p>
<ul>
<li><em>Bagged tree metrics</em> estimated based on a fraction of data:
<ul>
<li>rmse = 113.8576209</li>
<li>rsq = 0.2422997</li>
<li>computation time (on personal computer) was about 21600 seconds to tune and fit. Of note, the tunning was first tested based on a fraction of data and the resulting estimates of the hyperparamerters were used on the final prediction.</li>
</ul></li>
<li><em>Random forest metrics</em> based on final test:
<ul>
<li>rmse = 85.8138947197454</li>
<li>rsq = 0.452623773165699</li>
<li>computation time (on talapas, the University of Oregon virtual super computer) was exactly 22911.134 seconds to tune and fit.</li>
</ul></li>
<li><em>Boosted tree metrics</em> based on the tuned model applied to full training dataset:
<ul>
<li>rmse = 85.2073125</li>
<li>rsq = 0.4645603
= comoutation time (on talapas) was over 25,200 seconds to tune and fit.</li>
</ul></li>
</ul>
<p>While these metrics are not 100% standardized to compare given that some models were ran on a super computer while other were ran on a personal computer and some are based on metrics from final test data while others are based on full training data, this should not be an issue because change the dataset does not change the metrics dramatically. Overall, the estimates of metrics you get on a fraction of data are valid to give you an idea of your model performance. Furthermore, the differences in terms of timing between a super computer and a personal computer should be taken into account but since we will be manly comparing the random forrest to the boosted tree given how close their metrics are, this should not be a problem because they were both ran on Talapas, the super university computer.</p>
<p>Our findings of rmse and rsq are aligned with our general knowledge. That is, boosting usually outperforms random forest and bagging, but random forest is easier to implement and has a much better computational efficiency. In bagging, we number of predictores is already set, and we only tune two hyperparamerers, the number of decision <code>trees</code> to be created, and the minimum number of data points present in a terminal node, <code>min_n</code>. In random forest, we can tune many hyperparameters, including those we examined, namely the number of predictors sampled at each split <code>mtry</code>, the number of <code>trees</code> and <code>min_n</code>. However, in boosting, more tuning parameters are required, namely tree depth and learning rate based on itâ€™s performance <span class="math inline">\((rmse = 85.2073125)\)</span> to mention the a few based on our study. Therefore, our boosted tree model won the race althouhg it required more computational resources. That said, it is important to note that the difference in performance between random forrest and boosted tree models is not that big, so if computational efficiency is an issue, random forrest should be just fine in this case.</p>
<p>Finally, as mentioned in our boosted tree post, we wills submit our final model predictions to kaggle so that a quantitative indicator of prediction accuracy can be provided.</p>
<p>NOTE: We all computed predictions based on the test.csv dataset, because of the computational intensity of several of our datasets, in order to ease submission once we had selected the best model based on rmse.</p>
