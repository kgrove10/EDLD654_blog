---
title: Boosted Tree
author: 'Alexis Adams-Clark'
date: '2020-12-03'
slug: boosted-tree
categories: []
tags: []
---

In this post, we will outline how we fit a boosted tree model to our data. Most of our information on boosted trees comes from our lovely instructors [Daniel Anderson and Joe Nese](https://uo-datasci-specialization.github.io/c4-ml-fall-2020/), who taught a Machine Learning as part of the [Educational Data Specialization](https://education.uoregon.edu/data-science-specialization-educational-leadership) at the UO. 

## Boosted Tree Model Description

**What does the Boosted Tree Model do?**
"Boosting" is similar to "bagging," (see post on bagging [here](https://edld654finalproject.netlify.app/2020/12/01/bagged-tree/)) in that it's a general algorithm that can theoretically be used with many types of models. However, it's super common with tree models (see posts on bagging [here](https://edld654finalproject.netlify.app/2020/12/01/bagged-tree/) and random forest models [here](https://edld654finalproject.netlify.app/2020/12/02/random-forest/) that introduce the concepts of "decision trees"). In this post, we are going to use it here to make a Boosted Tree Model. Unlike "bagging" (which uses bootstrap resampling), boosted models are built in a sequential and iterative manner, and each subsequent model is fit to the residuals (error term) of the previous model. In other words, each tree builds upon information from the prior tree. The steps to creating a decision tree are below: (courtesy of Daniel Anderson and Joe Nese). 

First, a decision tree is fit the the data, which looks like: 
$f_1\left(x\right) = y$


Then, a second decision tree is fit to the residuals of the first, which looks like:
$h_1\left(x\right) = y - f_1\left(x\right)$


The trees are then added together to obtain an ensemble algorithm, which looks like:
$f_2\left(x\right) = f_1\left(x\right) + h_1\left(x\right)$


A new decision tree is then fit to the residuals of this model, which looks like:
$h_2\left(x\right) = y - f_2\left(x\right)$


This tree is then added to our ensemble again, which looks like:
$f_3\left(x\right) = f_2\left(x\right) + h_2\left(x\right)$


The process is repeated over and over again until we meet some specified stopping rule and we have our final model, which can be represented as: 
$$
f\left(x\right) =  \sum^B_{b=1}f^b\left(x\right)
$$

**Hyperparameters to be optimized**
Boosted trees have SO MANY hyperparameters that can be tuned for model optimization. There are 4 primary hyperparameters for standard Boosted Tree Models, including: 

*Tree Depth* - Like other tree models, tree depth is something to consider when designing a model. However, boosted trees typically involve shallow trees (3-6 splits). Boosted tree models with stumps (1 split) are also common. We probably want to stick with shallow trees because these are more computationally efficient, and deeper trees with a lot of splits put us at risk for overfitting to our training data. 

*Minimum n for terminal nodes* - Although this is indeed a hyperparameter that can be optimized in our boosted tree model, it's rarely necessary to tune because boosted tree models usually do not grow trees that are very deep. Right now, we are not going to worry about this hyperparameter. 

*n trees* - Unlike some other tree models, we want to really think about the number of trees that we fit in boosted tree models. This will determine how much our model learns overall. If we have too few trees, the model will underfit. If there are too many trees, the opposite will occur, and we will overfit. This often depends on what our tree depth is set to, so we want to pay close attention to the relationship between these two hyperparameters. If we have shallow trees that learn less, we might need a large number of total trees to balance that out. 

*Learning Rate* - Our learning rate (which is also called shrinkage) maps onto the size of the step taken down our gradient during each iteration (see below for a discussion of gradient descent). These values vary from 0-1, but rarely are above 0.3. Smaller values tend to be more helpful because we learn less from each individual tree, leading to more optimal generalizations that avoid overfitting to our data. However, like most things in machine learning, there is a delicate balance that must be struck. If we have too small of a learning rate, the model can be more computationally intensive and require a large number of trees. It is also sill subject to local minima that might exist across our gradient decent landscape. 

There are three additional hyperparameters for stochastic Boosted Tree Models that we can also consider (particularly if we want to add more randomness), including: 

*Subsample of rows* - We can actually tune our model to select a subsample of cases in the training data for each tree (usually between 50-80%)

*Subsample of columns for each tree* - We can also tune our model to select a sample of features (columns) to be used by each tree. 

*Subsample of columns for each level* - We can also tune our model to select a sample of features (columns) to be used by each level. 

*Subsample of columns for each node/split* - We can also tune our model to select a sample of features (columns) to be used by each split. 


If you can believe it, there are even **MORE** hyperparameters we can consider if we use `XGBoost` through `tidymodels` to run our boosted tree model.

*Loss Reduction* - controls the tree depth and specifies a minimum loss required for further splits. In other words, it stops growing the tree if the change in the cost function doesn't surpass a given threshold (see below for discussion of cost function). Although values range from 0 to infinity, values 1-20 are common.

*L1 and L2 penalities* - also possible to add if model is continuing to overfit

AND we can also vary our *early stopping* value. Although not a technical hyperparameter, it denotes a specific number of iterations that the algorithm will stop after if it's not improving predictions. This is another way to control the algoithm speed and protect against overfitting of the data. 

Because we are using `XGBoost` through `tidymodels`, we will specify a large number of trees and an early stopping rule. We will then tune the learning rate, then the tree specific parameters, and then the stochastic components. 


**Why did we select this model?**
We selected this model because, unlike random forests, which aggregate predictions across many deep trees (with a lot of splits), boosted models are slow learners. Boosted models start with a base learner that's a shallow decision tree with only a few splits. These models may be considered "weak learners" because they learn very slowly. However, they are far from weak in the literal sense of the word! For model prediction, slow learning can have a lot of benefits; usually, it helps us find the most optimal solution. 

Because of this, Boosted Tree Models are usually the best Out-Of-Box (OOB) models (and they win a lot of Kaggle data prediction competitions!). 


**Boosted Tree Model Assumptions**
Boosted Tree Models rely on an optimization algorithm known as Gradient Descent. Although gradient descent is a rather complex concept, it can be thought of as a way to evaluate our predictions against a "cost function," which then will move us in the direction with the "steepest descent." In a tree model, gradient descent is used to iteratively build each tree from the prior trees in the most optimal way. Models that use gradient descent specify a cost function, such as mean squared error. Then, the partial derivative of each model parameter is calculated according to the cost function, which will define our gradient landscape and allow us to move downhill in the steepest direction.  

The distance that our predictions move is influenced by the learning rate of our model, which is multiplied by these derivative values and then subtracted from the current parameter estimates. This learning rate is essentially the speed at which the algorithm steps downhill. 

The model continues to move down the gradient in an iterative fashion to minimize our model error until our model can't improve any more! During each iteration, we will take a step along the gradient in the most optimal direction to reduce the error in our predictions. Luckily, we don't have to do any of this by hand ourselves because `tidymodels` will handle this for us! Bless you, `tidymodels`! 


## Fitting our Boosted Tree Model

**Model Fitting Procedures** 

A description of our data preparation procedures can be found [here](https://edld654finalproject.netlify.app/2020/11/30/data-description/). The model fit was completed using the University of Oregon's "Talapas" Supercomputer, so while some example code will be included with this post, computations were conducted elsewhere, and the full code can be found on our [GitHub here](https://github.com/kgrove10/EDLD654_blog/blob/main/static/boosted_tree_model). Unfortunately, although I would love to share the .RDS file that talapas provides us with after the model is done running, I couldn't upload it to github because it was too big, and github wanted me to upgrade my storage. 

We will start out with a default Boosted Tree Model with no tuned hyperparameters. The arguments defined are based on recommendations from Daniel Anderson and Joe Nese. 

```{r boost, eval = FALSE}
set.seed(500)

#default model without tuning
mod <- boost_tree() %>% 
  set_engine("xgboost", nthreads = parallel::detectCores()) %>% 
  set_mode("regression") %>% 
  set_args(trees = 5000, #number of trees in the ensemble
           stop_iter = 20, #the number of iterations without improvement before stopping
           validation = 0.2,
           learn_rate = 0.1) #the rate at which boosting algoirithm adapts at each iteration

#translate model, so we get a sense of what it looks like
translate(mod_boost)

#create workflow for default boosted model
wf_df <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod)

#fit model
fit_default <- fit_resamples(
  wf_df, 
  train_cv, 
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))

#collect metrics
bt_def_met <- fit_default %>%
  collect_metrics() 

bt_def_met %>%
  write.csv("./BTTuneMetricsDefault.csv", row.names = FALSE)

```

The metrics for the default model are: 

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(rio)
library(here)

tunedres <- import(here("static", "boosted_tree_model", "BTTuneMetricsDefault.csv"))

knitr::kable(tunedres)

```

If we were happy with this (and didn't want to tune hyperparameters), we could then apply this our workflow to our last split with the following code: 

```{r eval = FALSE}
test_fit <- last_fit(wf_df, edu_split)
test_metrics <- test_fit$.metrics

test_metrics %>%
  write.csv("./BTTestMetrics1.csv", row.names = FALSE)
```

When our default boosted model is applied to our full training set, we get the following results: 

```{r echo = FALSE}
testres <- import(here("static", "boosted_tree_model", "BTTestMetrics1.csv"))

knitr::kable(testres)
```

And then we can make our predictions on unseen data, and submit that to Kaggle with the following code: 

```{r predict, eval = FALSE}
#make predictions on test.csv using this final workflow
full_test <- read_csv("data/test.csv",
                      col_types = cols(.default = col_guess(), 
                                       calc_admn_cd = col_character()))
#str(full_test)

#join with ethnicity data
full_test_eth <- left_join(full_test, ethnicities) #join with FRL dataset
#str(full_test_eth)

#join with frl
full_test_FRL <- left_join(full_test_eth, frl)

nrow(full_test_FRL)

#workflow
fit_workflow <- fit(wf_df, frl_fulltrain)

#use model to make predictions for test dataset
preds_final <- predict(fit_workflow, full_test_FRL) #use model to make predictions for test dataset

head(preds_final)

pred_frame <- tibble(Id = full_test_FRL$id, Predicted = preds_final$.pred)
head(pred_frame, 20)
nrow(pred_frame)

#create prediction file
write_csv(pred_frame, "fit_bt.csv")

```

THEN SUBMIT TO KAGGLE! 




But wait......

What if we really wanted to try and reduce the rmse by tuning the hyperparameters?

Let's try to tune the learning rate of the model to reduce the rmse even more using the following code:

```{r boosted tuning, eval = FALSE}
set.seed(500)

#tuned model
tune_lr <- mod %>% 
  set_args(trees = 5000, #number of trees in the ensemble
           stop_iter = 20, #the number of iterations without improvement before stopping
           validation = 0.2,
           learn_rate = tune()) #the rate at which boosting algoirithm adapts at each iteration

#let's get a sense of what this tuned model looks like
translate(tune_lr)

#create workflow for tuned model
wf_tune_lr <- wf_df %>% 
  update_model(tune_lr)
```


```{r boost4, eval = FALSE}
#grid for tuning learning rate
grd <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

#fit model w/ tuned learning rate
tune_tree_lr <- tune_grid(
  wf_tune_lr, 
  train_cv, 
  grid = grd,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))

#let's collect metrics
#collect metrics
bt_grid_met <- tune_tree_lr %>%
  collect_metrics() 

bt_grid_rsq <- bt_grid_met  %>%
  filter(.metric == "rsq") %>%
  arrange(.metric, desc(mean)) %>%
  slice(1:5)

bt_grid_rmse <- bt_grid_met %>%
  filter(.metric == "rmse") %>%
  arrange(.metric, mean) %>%
  slice(1:5)

bt_grid_metrics <- rbind(bt_grid_rsq, bt_grid_rmse) 

bt_grid_metrics %>%
  write.csv("./BTTuneMetricsGrid.csv", row.names = FALSE)

```

It looks like we got the following metrics for our model:

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(rio)
library(here)

tunedres <- import(here("static", "boosted_tree_model", "BTTuneMetricsGrid.csv"))

knitr::kable(tunedres)
```

It looks like a learn rate of 0.0828310 gives us our best rmse and rsq values! Let's create a plot using the following code: 

```{r plot, eval = FALSE}
#Let's plot
to_plot <- tune_tree_lr %>% 
  unnest(.metrics) %>% 
  group_by(.metric, learn_rate) %>% 
  summarize(mean = mean(.estimate, na.rm = TRUE)) %>% 
  filter(learn_rate != 0.0001) 

highlight <- to_plot %>% 
  filter(.metric == "rmse" & mean == min(mean)) %>%
  ungroup() %>% 
  select(learn_rate) %>% 
  semi_join(to_plot, .)

ggplot(to_plot, aes(learn_rate, mean)) +
  geom_point() +
  geom_point(color = "#de4f69", data = highlight) +
  facet_wrap(~.metric, scales = "free_y")

```


```{r viz, echo=FALSE}
blogdown::shortcode("figure", src ='"/boosted_tree_model/BTMetricsGrid_Tuned.pdf"', alt = "Tuned Metrics Visualization")

```


If we wanted to now finalize our model and apply it to our test data to make predictions, we would use the following code. 

```{r final model, eval = FALSE}

#Now let's select best model and finalize our workflow
best_rmse <- tune_tree_lr %>% 
  select_best(metric = "rmse")

bt_wf_final <- finalize_workflow(
  wf_tune_lr,
  best_rmse)

#fit to test set
test_fit <- last_fit(bt_wf_final, edu_split)
test_metrics <- test_fit$.metrics

test_metrics %>%
  write.csv("./BTTestMetrics_Tuned.csv", row.names = FALSE)
```

**Model Fitting Results** 

Our final workflow applied to the full training dataset gives us the following results:

```{r , echo = FALSE}
testres <- import(here("static", "boosted_tree_model", "BTTestMetrics_Tuned.csv"))

knitr::kable(testres)
```



Then we would follow a similar procedure as above to submit to predict on our unseen data and submit to Kaggle! 



SPECIAL NOTE: However......

If we wanted to tune other hyperparameters because we are really ambitious, we could use the following code (although we did not execute this code for our model predictions because it just took way too long!!!!). 

```{r boost tuning 2, eval = FALSE}
set.seed(500)

#new hyperparameters to tune
tune_depth <- tune_lr %>% 
  finalize_model(select_best(tune_tree_lr, "rmse")) %>% 
  set_args(tree_depth = tune(), #tree depth - the maximum depth of a tree
           min_n = tune()) #min_n - the minimum number of data points in a node thst is required for the node to be split further

#update workflow
wf_tune_depth <- wf_df %>% 
  update_model(tune_depth)

#update grid
grd <- grid_max_entropy(tree_depth(), min_n(), size = 30)

#fit model
tune_tree_depth <- tune_grid(
  wf_tune_depth, 
  train_cv, 
  grid = grd,
  metrics = metric_set(rmse, rsq)

#plot
autoplot(tune_tree_depth)

#show best
show_best(tune_tree_depth, "rmse")
```


If we wanted to tune regularization, randomness, and learning rate one more time, we could use this code (although we did not do this for our model predictions because it just took way too long!)

```{r tune 3, eval = FALSE}

tune_reg <- tune_depth %>% 
  finalize_model(select_best(tune_tree_depth, "rmse")) %>% 
  set_args(loss_reduction = tune()) #loss_reduction - the reduction in the loss function required to split further

#update workflow
wf_tune_reg <- wf_df %>% 
  update_model(tune_reg)

#update grid
grd <- expand.grid(loss_reduction = seq(0, 100, 5))

#run model again
tune_tree_reg <- tune_grid(
  wf_tune_reg, 
  train_cv, 
  grid = grd)

autoplot(tune_tree_reg)

#show best
show_best(tune_tree_reg, "rmse")

#tune randomness
tune_rand <- tune_reg %>%
  finalize_model(select_best(tune_tree_reg, "rmse")) %>% 
  set_args(mtry = tune(), #mtry - the number of predictors that will be randomly sampled at each split when creating the tree models
           sample_size = tune()) #sample_size - the amount of data exposed to the fitting routine

#update workflow
wf_tune_rand <- wf_df %>% 
  update_model(tune_rand)

#update grid
grd <- grid_max_entropy(finalize(mtry(), 
                        juice(prep(rec))), 
                        sample_size = sample_prop(), 
                        size = 30)

tune_tree_rand <- tune_grid(
  wf_tune_rand, 
  train_cv, 
  grid = grd)  

#plot
autoplot(tune_tree_rand)

#show bet metrics
show_best(tune_tree_rand, "rmse")


#check learning rate one more time
check_lr <- tune_rand %>% 
  finalize_model(select_best(tune_tree_rand, "rmse")) %>% 
  set_args(learn_rate = tune())

#update workflow
wf_final_lr <- wf_df %>% 
  update_model(check_lr)

#update grid
final_lr <- tune_grid(wf_final_lr, cv, grid = 30)

#plot
autoplot(final_lr)

#show best
show_best(final_lr, "rmse")

```


