---
title: Random Forest
author: Kivalina Grove
date: '2020-11-30'
slug: random-forest
categories: []
tags: []
---

In this post, we will outline how we fit a random forest model to our data. 

## Random Forest Model Description

**What does the Random Forest Model do?**

To understand a random forest model, we first have to understand its building blocks, decision trees.  Decision trees can be used for either regression or classification problems (here, we're using them to address a regression problem of predicting student scores). A decision tree is essentially a series of splits in the data, where is split at a series of nodes based on the feature that makes the separated groups to as different as possible from each other (this feature at this first split is called the "root node"). Data from either side of this split are then split again on the next feature that makes that data as different as possible, and so on (so called interior or internal nodes) until you reach some terminal number of observations (the leaf, or terminal node). Thus, given an observation,  you can move through this decision tree based on the features of that observation and the splits to predict some result (regression or classification). 

A random forest model uses these decision trees in aggregate (sometimes called ensemble learning), by combining many decision trees into one model. In essence, this model uses the "wisdom of crowds" notion - while individually, decision trees may not be great predictors, in combination, they are successful. The random forest model is a bagging (bootstrap aggregation) technique, meaning it runs many decision trees in parallel on bootstrapped copies of the training data, and then outputs as the predicted model the mean prediction of each of these individual trees. However, one major benefit of the the random forest model is that it also utilizes *split-variable randomization* to reduce correlation between trees. For each of the constitutive decision trees in a random forest, at each split, only a random sample of the features/predictors are available to utilize. This means that if you have a (or a couple) strong predictor(s) in the dataset, when using decision trees alone, you're likely to see the (especially initial) splits dominated by these strong predictors. When using random forests, since these strong predictors won't always be present in the randomly selected subset of the predictors utilized by the constitutive decision trees, splits are made on other predictors as well, not just the most dominant ones, and thus the decision trees are less correlated with each other as compared to bagged trees, which increases the reliability of their aggregate result. (see also [Chakure, 2019](https://medium.com/swlh/random-forest-and-its-implementation-71824ced454f), [Yiu, 2019](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)).

**Hyperparameters to be optimized**

The random forest model provides very good "out-of-the-box" performance, meaning that its default hyperparameter values tend to produce good results.  That said, there are hyperparameters that can be tuned in the random forest model. We will tune three of them, `mtry`, `trees` and `min_n`:

- `mtry` represents the number of predictors that will be sampled randomly at each split when creating the constitutive decision trees. 
- `trees` represents the number of individual decision trees to be created and then aggregated across. 
- `min_n` represents the minimum number of data points present in a node to terminate that node - essentially, how specific should we get?  How many data points should be present in a terminal node (or leaf) when we cease splitting our data? 

Other hyperparameters we did not tune but that could be tuned for random forest models include the sampling (sapling? :) ) scheme (the default is bootstrapping, where observations are tuned with replacement) and the split rule (the default is the split that maximizes the Gini impurity). 


**Why did we select this model?**

We selected this model as one of our 3 fits because we wanted to explore several models that utilize decision trees. Regression using decision trees in general does a good job of capturing non-linear features in the data as compared to simpler linear regression. Additionally, random forest models (along with boosted tree models, which we also examine), generally have pretty good out of the box performance, and we thought it would be interesting to compare their performances when tuned. 

When is it appropriate to use random forest models? As outlined above and below, random forest models are generally a fairly solid choice, because they don't make any assumptions about the underlying distribution of the data. The are also less influenced by outliers than other models (because they are built on averaged decision trees, so extreme values don't impact the entire model), and aren't as vulnerable to multi-collinearity, since a split on either of these highly correlated features would essentially use up the information (predictive power) that would be provided by the other highly correlated feature. However, this can also be an issue with random forest models. Because one of the multicollinear variables is used for a split whilst the other is not, it could erroneously appear that one of these highly correlated variables is much more important than the other, when in fact they are very close in their predictive power for the dependent variable. While this effect is mitigated somewhat by the use of split-variable randomization, it is not completely eliminated ([Saabas, 2014](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)). In general, one area of concern with random forest models is their difficulty of interpretation in terms of the individual impact of features on the dependent variable. In this case, we're more interested in our ability to successfully and accurately predict student scores than understanding the individual feature contribution to these scores, so the random forest model is a good choice, especially in conjunction with the bagged and boosted tree model as our goal here is to compare their performance. 

**Random Forest Model Assumptions**

Because the random forest model is built out of decision trees, it inherits their assumptions, which are fairly simple. Because decision trees function to split the data, they have no probabilistic model, and therefore we do not need to make any assumptions about the underlying data distribution. Random forest models rely upon bootstrap aggregation (bagging), which also does not make any assumptions about the underlying data distribution, but does rely upon the assumption that the data was collected in such a manner that the sampling was representative, meaning there was no systematic bias in the sample collection, and the characteristics of the sample accurately represent that of the larger group, both in presence, and in proportion represented ([Chatterjee, 2019](https://towardsdatascience.com/all-the-annoying-assumptions-31b55df246c3))


**Model Performance Evaluation**

We will evaluate the performance of this model after training by applying it to our held out test split from the training data (also called the validation set). We will examine rmse (appropriately, the *root* mean squared error, the standard deviation of the residuals), rsq (R-squared, the proportion of variance in the dependent variable explained by the variables in our regression model), and huber-loss (a loss function that represents absolute error, and is less sensitive to outliers than the mse and rmse), but largely rely upon the rmse as our single evaluator of the performance of this and the other tree models we present in this blog in order to select our final model.  


## Fitting our Random Forest Model

**Model Fitting Procedures** 

To fit our random forest model, we first imported and split the data, and applied our recipe, as described [here](https://edld654finalproject.netlify.app/2020/11/30/data-description/). The model fit was completed using the University of Oregon's "Talapas" Supercomputer, so while some example code will be included with this post, computations were conducted elsewhere, and the full code can be found on our [GitHub here](https://github.com/kgrove10/EDLD654_blog/blob/main/static/random_forest_model/RF_Talapas.R). 

First, we started by coding a random forest model. For this, we utilized the function `rand_forest()` from the `parsnip` package. In this code, we set our engine to `"ranger"`, which offers fast implementation of random forests. Since it supports both classification and regression, we need to specify regression in the piped function `set_mode()`. While setting our engine, we also specify several arguments from ranger, including the number of threads, or the number of distinct processes running in parallel. In this case, we set it equal to the number of cores, so each core is executing one thing independently. If running locally, we could run `parallel::detectCores()`, but since we're running this code on talapas, we manually set it to the number of cores talapas possesses, 8. The argument `importance = "permutation"` is set to specify the variable importance mode. The default is `impurity` which is the Gini index, or the probability of a variable being wrongly classified when it is randomly chosen, but we set it here to be `permutation` instead.  The issue with impurity-based feature importance is that it can inflate the importance of numeric features over categorical features. Since we have both in our model, we chose to use `permutation`, which is more reliable that impurity, because it instead calculates the importance of a feature based on the increase in the model's prediction error after permuting the feature. The downside of permutation is that it is more computationally demanding, and potentially biased toward collinear predictors. 

```{r eval = FALSE}
cores <- 8

rf_def_mod <- rand_forest() %>%
  set_engine("ranger",
             num.threads = cores,
             importance = "permutation",
             verbose = TRUE) %>%
  set_mode("regression")
```


Next, we wanted to tune three hyperparameters utilized in our random forest model, `mtry`, `trees` and `min_n` (described in detail above), so we set these hyperparameters to be tuned.

```{r eval = FALSE}
rf_tune_mod <- rf_def_mod %>%
  set_args(
    mtry = tune(),
    trees = tune(),
    min_n = tune()
  )

translate(rf_tune_mod)
```

We also translated our tuning model, which gives us a more interpretable printout of the settings we have established for our tuning model, which looks like this:

    Random Forest Model Specification (regression)
    
    Main Arguments:
      mtry = tune()
      trees = tune()
      min_n = tune()
    
    Engine-Specific Arguments:
      num.threads = cores
      importance = permutation
      verbose = TRUE
    
    Computational engine: ranger 
    
    Model fit template:
    ranger::ranger(formula = missing_arg(), data = missing_arg(), 
        case.weights = missing_arg(), mtry = tune(), num.trees = tune(), 
        min.node.size = tune(), num.threads = cores, importance = "permutation", 
        verbose = TRUE, seed = sample.int(10^5, 1))

Next, we created a workflow, which is an object that can bundle together pre-processing  modeling, and post-processing requests. In this case, we are combining our recipe with our parsnip model specification, so that we can then prepare the recipe and fit the model using a single step, calling `fit()`. We also specify the three metrics we will evaluate our tuned models on, `rmse`, `rsq` and `huber_loss` (described above) using the `metric_set()` function. 

```{r eval = FALSE}
rf_tune_wflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_tune_mod)

metrics_eval <- metric_set(rmse, 
                           rsq, 
                           huber_loss)
```

We then fit the tuning model, using the argument `tune_grid()`.  Here, we are using a space-filling design of 20 values for each of our three tuning parameters, which we specify using the workflow object we created above. We also set `extract` to extract our ranger model for every fold.

```{r eval = FALSE}
rf_tune_res <- tune_grid(
  rf_tune_wflow,
  train_cv,
  tune = 20,
  metrics = metrics_eval, #from above - metrics of rmse, rsq, huber_loss
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))
```

Once this runs (it took about 2.5 hours on talapas), we can collect the metrics from our model. The code below gathers the best 5 models based on `rsq`, `rmse`, and `huber_loss` (note that "best" for rmse and huber loss is lower values, while "best" for rsq is higher values, and thus we reverse the order before slicing the top values for rmse). We then bind these values together and save them as a .csv so I can include the results in this blog post. 

```{r eval = FALSE}
rf_tune_met <- rf_tune_res %>%
  collect_metrics() 

rf_tune_rsq <- rf_tune_met %>%
  filter(.metric == "rsq") %>%
  arrange(.metric, desc(mean)) %>%
  slice(1:5)

rf_tune_rmse <- rf_tune_met %>%
  filter(.metric == "rmse") %>%
  arrange(.metric, mean) %>%
  slice(1:5)

rf_tune_hl <- rf_tune_met %>%
  filter(.metric == "huber_loss") %>%
  arrange(.metric, mean) %>%
  slice(1:5)

rf_tune_metrics <- rbind(rf_tune_rsq, rf_tune_rmse, rf_tune_hl) 

rf_tune_metrics %>%
  write.csv("./RFTuneMetrics.csv", row.names = FALSE)
```

The "top" model results for our tuned random forest model are below:

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(rio)
library(here)

tunedres <- import(here("static", "random_forest_model", "RFTuneMetrics.csv"))

knitr::kable(tunedres)
```

We can also visualize these results using the following code to generate a plot of the tuned metrics (and save it to include in this blog):
```{r eval = FALSE}
rf_tune_res %>%
  autoplot() +
  geom_line()

ggsave("RFTunedMetrics.pdf",
       plot = last_plot(),
       scale = 1)
```


```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/random_forest_model/RFTunedMetrics.pdf"', alt = "Tuned Metrics Visualization")
```

As we can see from these results, our best model, according to all three of our metrics is Model 01, with `mtry` = 5, `trees` = 1323, and `min_n` = 38. 

This is in line with the general guidelines for these predictors: `mtry` between 2 and `p` (representing the number of predictors in the data), `trees` at least 1000, and `min_n` starting between 1 and 10 and adjust depending on impact to accuracy and run time. Here, our `min_n` is much larger, but as we can see from the visualization, it improves as the value increases. In fact, we could consider increasing beyond the tuned value of 38, since it still appears to be decreasing, but in the interest of computational expense, we will stick with this selected value of 38. 

We then select the best results using `select_best`, based on the `rmse` metric (although in this case, all our metrics are in agreement), and will use these hyperparameter values going forward.  

```{r eval = FALSE}
rf_best <- select_best(rf_tune_res, metric = "rmse")
```

Next, we will finalize our workflow, based on this best result, using the function `finalize_workflow()`. This will add our best hyperparameters we selected above to the workflow we set up above. 

```{r eval = FALSE}
rf_wf_final <- finalize_workflow(
  rf_tune_wflow,
  rf_best)
```

Now, our workflow looks like this:

Random Forest Model Specification (regression)
    
    Main Arguments:
      mtry = 5
      trees = 1323
      min_n = 38
    
    Engine-Specific Arguments:
      num.threads = cores
      importance = permutation
      verbose = TRUE
    
    Computational engine: ranger 
    
    Model fit template:
    ranger::ranger(formula = missing_arg(), data = missing_arg(), 
        case.weights = missing_arg(), mtry = 5, num.trees = 1323, 
        min.node.size = 38, num.threads = cores, importance = "permutation", 
        verbose = TRUE, seed = sample.int(10^5, 1))



**Model Fitting Results**

Finally, in order to get the results of our model with these tuned hyperparameter values, we will apply this model to our full training dataset, `edu_split`.

We do this using the following code, which applies the final workflow we created above to our full training dataset using the function `last_fit()`. From this fit, we then extract just the metrics (and save them) in order to compare the results of this random forest model with our bagged tree and boosted tree model in order to choose a final model to make student score predictions from the test data (`test.csv`) and to submit to kaggle. 

```{r eval = FALSE}
test_fit <- last_fit(rf_wf_final, edu_split)
test_metrics <- test_fit$.metrics

test_metrics %>%
  write.csv("./RFTestMetrics.csv", row.names = FALSE)
```

Our final workflow applied to the full training dataset gives us the following results:

```{r echo = FALSE}
testres <- import(here("static", "random_forest_model", "RFTestMetrics.csv"))

knitr::kable(testres)
```

In order to best compare our models, it's also important to note how long tuning and fitting this model took. Using talapas, this process took 22911.134 seconds, or approximately `r round(22911.134/60/60, 2)` hours, which is quite computationally expensive!  You can read about our final model choice and fit [here](https://edld654finalproject.netlify.app/2020/12/03/final-model-fit/). 












