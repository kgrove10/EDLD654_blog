---
title: Intro & Data Description
author: 'Alexis Adams-Clark, Kivalina Grove, and Ouafaa Hmaddi'
date: '2020-11-30'
slug: data-description
categories: []
tags: []
---

![There are so many types of trees! ](https://www.homestratosphere.com/wp-content/uploads/2019/07/Two-categories-of-trees.jpg)

In this blog, we're going to walk through the basics for getting off the ground with machine learning using primarily the {tidymodels} package. Specifically, we will demontrate the application of three different tree-based methods for predicting student test scores. For further information about the {tidymodels}, you can visit [this page](https://www.tidymodels.org/).

Before we start fitting models, let's talk more about the data we'll be using throughout our posts. 

## Intro to the Data

The main data we are using for this project, (`train.csv` and `test.csv`) come from an annual test students across the country take between grades 3-8 in reading and math. The 198,426 data points we utilize here come specifically from the state of Oregon, and is simulated rather than actual data, although the distribution of this simulated data is very similar to that of the real data. This data can be accessed from our [GitHub](https://github.com/kgrove10/EDLD654_blog/tree/main/static/data) or from the [Kaggle competition](https://www.kaggle.com/c/edld-654-fall-2020/data) for this course. 

We also joined a couple additional data sets to this data for modeling purposes. The first, `fallmembershipreport_20192020.xlsx` is a student enrollment report that provides information about the K-12 students who are enrolled on the first day of October of each year (this data is for the 2019-2020 school year, from October 1st, 2019). It can be accessed from the [Oregon Department of Education Website](https://www.oregon.gov/ode/reports-and-data/students/Pages/Student-Enrollment-Reports.aspx). The students included in this sample include all students enrolled in public schools and programs (including regular, alternative, charter, and other types of schools and programs). The report also includes students attending private schools if the student was placed there by a private entity or financed with public funds. No student is double-listed, that is, no student is enrolled in more than one school or district. Next, we join data from the [National Center for Educational Statistics (NCES)](https://nces.ed.gov/ccd/files.asp#Fiscal:2,LevelId:7,SchoolYearId:32,Page:1) which provides the number of students by school who are eligible for various free or reduced lunch programs. We will filter this data for just those students from Oregon. We join this data with [data on student counts](https://raw.githubusercontent.com/datalorax/ach-gap-variability/master/data/achievement-gaps-geocoded.csv), or the number of students in each school across grades.

Example code to read in and join this data is as follows. Because of the size of this data and in the interest of not exhausting the vector memory of this blogdown site, we will present example code and output below without actually evaluating it. Reproducible code and output for each of our models can be found on our [GitHub](https://github.com/kgrove10/EDLD654_blog/tree/main/static) under each of the three model folders. 

**Getting started: Loading and Cleaning the Data**

First, we will load some libraries into our environment that we will be using throughout this post. The classic {tidyverse} and for wrangling and {rio} for importing, our new {tidymodels} and the {skimr} package to help us explore our data and {knitr} to help us visualize tables.

```{r eval = FALSE}
library(tidyverse) # manipulating data
library(rio) #importing data
library(tidymodels) 
library(skimr) # data visualization
library(knitr) #table visualization 
``` 

Next, we're going to read in the data. When we are importing the data, we will also sightly clean it. For example, with the fall membership 2019-2020 student enrollment report, we will update the names of columns to be in snake case to both match the names of our variable names in our main, `train.csv` data (e.g. "Attending School ID" becomes "attnd_schl_inst_id"), and to make these variables easier to work with in R.  When importing our free and reduced lunch data, we clean names, and also replace missing data with 0s for student counts. Below, we will go over each of the variables present in our data. 

```{r eval = FALSE}
set.seed(500)
# import our training data, remove the classification column, since we are interested in score, not student classification
full_train <- read_csv("data/train.csv",
                       col_types = cols(.default = col_guess(), 
                                        calc_admn_cd = col_character()))  %>% 
  select(-classification)
  
#import our fall membership student enrollment report
sheets <- readxl::excel_sheets("data/fallmembershipreport_20192020.xlsx")
ode_schools <- readxl::read_xlsx("data/fallmembershipreport_20192020.xlsx", sheet = sheets[4])
str(ode_schools)

ethnicities <- ode_schools %>% select(attnd_schl_inst_id = `Attending School ID`,
                                      sch_name = `School Name`,
                                      contains("%")) %>%
  janitor::clean_names()

names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

#read in our free and reduced lunch data
frl <- rio::import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
                   setclass = "tbl_df")  %>%
  janitor::clean_names()  %>%
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>%
  mutate(student_count = replace_na(student_count, 0))  %>%
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>%
  janitor::clean_names()  %>%
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades, filter for oregon schools, 
# and change type of id to be numeric
stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv", setclass = "tbl_df")  %>%
  filter(state == "OR" & year == 1718)  %>%
  count(ncessch, wt = n)  %>%
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl <- left_join(frl, stu_counts)
```

Next, we need to join these three datasets together so we can use them to fit our tree models.  Before we do this, we need to make sure that the "id" we are going to join by is unique across all the datasets, meaning that it doesn't correspond to more than one row. The following function, called `unique_id()` from the work of [Thoen (2017)](https://edwinth.github.io/blog/unique_id/) will check if the variable we want to join our datasets by is a unique id. 

```{r eval = FALSE}
#function to check id is unique
unique_id <- function(x, ...) {
  id_set <- x %>% select(...)
  id_set_dist <- id_set %>% distinct
  if (nrow(id_set) == nrow(id_set_dist)) {
    TRUE
  } else {
    non_unique_ids <- id_set %>% 
      filter(id_set %>% duplicated()) %>% 
      distinct()
    suppressMessages(
      inner_join(non_unique_ids, x) %>% arrange(...)
    )
  }
}
```

When we run this function on our `ethnicities` data (from fall membership report), we see that the id variable `attnd_schl_inst_id` is not unique. To remedy this, without knowing more about how this data was collected, and which ethnicity values are correct, we will average across observations for each school to create one aggregate observation for each school that appears more than once. Specifically, we will group by school id, and average across the recorded ethnicity values for each ethnic group using the following code:

```{r eval = FALSE}
#make ethnicities have attnd_schl_inst_id as a unique identifier 
ethnicities <- ethnicities %>%
  group_by(attnd_schl_inst_id) %>%
  summarize(p_american_indian_alaska_native = mean(p_american_indian_alaska_native),
            p_asian = mean(p_asian),
            p_native_hawaiian_pacific_islander = mean(p_native_hawaiian_pacific_islander),
            p_black_african_american = mean(p_black_african_american),
            p_hispanic_latino = mean(p_hispanic_latino),
            p_white = mean(p_white),
            p_multiracial = mean(p_multiracial))

ethnicities %>% unique_id(attnd_schl_inst_id)
```

Now we can use the `unique_id()` function again, and it prints TRUE, indicating that attnd_schl_inst_id is now a unique identifier for our `ethnicities` data. 

Next, we can join all our data together: first, our `full_train` data from `train.csv` and our `ethnicities` data, and then our free and reduced lunch data, `frl`. Note that for both of these joins, we utilized left joins, in order to preserve the information in our left hand dataset, here, the data from `train.csv`, since our interest is to add additional data on ethnicities, student counts, and free and reduced lunch program data by school to each of our original observations. 

```{r eval = FALSE}
#Join ethnicity data and training data
full_train <- left_join(full_train, ethnicities)


# add frl data to train data
frl_fulltrain <- left_join(full_train, frl)
```

Now our three datasets are all loaded, cleaned, and joined together to make one cohesive data set we will use for all three of our tree-based models. Before we move on to preparing the data for modeling, let's explore and describe the variables present in our joined data set, `frl_fulltrain`. 

Each row of our data corresponds to an individual student observation. However, the data contained in each row includes both student-level variables, such as the student's grade, gender, ethnicity, and enrollment in various programs, as well as school-level and district-level data, such as school ethnicity proportions and the proportion of students who qualify for free or reduced lunch programs. A detailed data dictionary for the initial `train.csv` data can be found [here](), but we will attmpet to provide a brief description and overview of the 46 variables present in our `frl_fulltrain` data each of our three tree-based models will rely upon. 



After joining, the data contains both student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch, etc.), all of which will be included for each 3 of our {tidymodels} tree-based examples.

#Explore data

We will use the `skim()` function from {skimr} to take a closer look at the distribution of our variables. Many numeric predictors are non-normal (see histograms below), but this is not a problem since tree-based methods are robust to non-normality.

```{r eval = FALSE}
frl_fulltrain %>% 
  select(-contains("id"), -ncessch) %>%  # remove ID and irrelevant variables
  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% # convert test date to date
  modify_if(is.character, as.factor) %>%  # convert character vars to factors
  skim() %>% 
  select(-starts_with("numeric.p")) # remove quartiles
```

![T1](https://www.dropbox.com/s/vdxzvee369nx24q/Slide1.PNG?dl=0)
![T2](https://www.dropbox.com/s/qxgz5gzdw273g33/Slide2.PNG?dl=0)

We will also use {corrplot} to better visualize the relationships among the numeric variables shown in the table above. However, this is a limited visual given that most of our predictors are acutally categorical.

```{r eval = FALSE}
#look at relations between variables
frl_fulltrain %>% 
  select(-contains("id"), -ncessch) %>% 
  select_if(is.numeric) %>% 
  select(score, everything()) %>% 
  cor(use = "complete.obs") %>% 
  corrplot::corrplot()
```

![plot](https://www.dropbox.com/s/8a8a0u2aszufiyd/VariableRelations.png?dl=0)

#Split the data 

First, we split the data into two separate sets: a “training” set and a “testing” set. The training set is used to train a model and, if desired, to adjust (“tune” in ML language) the model’s hyperparameters before evaluating its final performance on the test data. This helps us assess the “out of sample” accuracy and limit overfitting to the training set. Overfitting refers to a model that models the training data too well. It occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize. 

To split our data, we use the initial_split() function from the {rsample} package. The default is that 75% of the data goes to the training set and 25% to the test set, but this can be adjusted with the prop argument. 

To prevent overfitting, we resample the data using vfold_cv() which outputs k-fold cross-validated versions of the training data, where k is the number of times we resample. By setting k to 6 data sets, we get a better estimate of the model’s out-of-sample accuracy. This decreases bias from overfitting. And of course, we should always remember to set the seeds first.

```{r eval=FALSE}
set.seed(500)
edu_split <- initial_split(frl_fulltrain)
train <- training(edu_split)
test <- testing(edu_split)

train_cv <- vfold_cv(train, strata = "score", v = 6)
```

#Preparing and baking the recipe (i.e. Feature engineering)

Before we add in our data to the model, we are going to set up an object that pre-processes our data. This is called a recipe. To create a recipe, we will first specify a formula for our model, indicating which variable is our outcome and which are our predictors. Here, we will use all variables other than score as predictors. We then specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role, encode, omit or impute missing data, and/or model other feature engineering steps . 

Our recipe does the following:

* Sets score as the outcome, and models all other variables in the dataset
* Transforms dates into numberic variables
* Removes variables that contain "bnch"
* Sets the role of ID variables (including ncessch) 
* Imputes missing data
* Removes zero-variance and near-zero variance predictor variables 
* Dummy codes all nominal predictor variables

Note: The order of these operations matters. If you end up with a rank-deficient model you need to revise your recipe. 

A complete list of possible pre-processing steps can be found [here](https://www.tidymodels.org/find/recipes/)

```{r eval=FALSE}
rec <- recipe(score ~., train) %>%
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
              time_index = as.numeric(tst_dt)) %>%
  step_rm(contains("bnch")) %>%
  update_role(tst_dt, new_role = "time_index") %>%
  update_role(contains("id"), ncessch, new_role = "id") %>% #removed sch_name
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal(), -all_outcomes()) %>%
  step_rollimpute(all_numeric(), -all_outcomes(), -has_role("id"), -n) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id")) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
  step_dummy(all_nominal(), -has_role(match = "id"), -all_outcomes(), -time_index) %>%
  step_nzv(all_predictors()) %>%
  step_interact(terms = ~ starts_with('lat'):starts_with("lon"))

prep(rec) 
```

#Setting up a model and workflow

There are a few core elements that you will need to specify to create a model. This apllies to all models. 

* The type of model: Here we will be focusing on tree-based models. 
* The engine: `set_engine()` calls the package to support the model you specified.
* The mode: `set_mode()` indicates the type of prediction you would like to use in your model. You choose between regression and classification. In our case, we will be choosing regression because we are looking to predict student scores, which is a continuous predictor.
* The arguments: `set_args()` allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered.

The final step here is to creat a workflow. A workflow combines all the elements we set up to create a cohesive framework, called a workflow, so we can run our desired models following the same exact framework. 