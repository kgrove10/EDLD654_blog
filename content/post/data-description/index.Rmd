---
title: Intro & Data Description
author: 'Alexis Adams-Clark, Kivalina Grove, and Ouafaa Hmaddi'
date: '2020-11-30'
slug: data-description
categories: []
tags: []
---

In this blog, we are going to describe how we used machine learning with the [tidymodels](https://www.tidymodels.org/) package (and a couple other packages we'll introduce later) to fit three different tree-based models to predict student test scores (bagged trees, random forests, and boosted trees).

Before we get to fitting our models though, we'll introduce our data and the preprocessing and feature engineering steps we took to prepare our data to fit these three models.

## Intro to the Data

The main data we are using for this project, (`train.csv` and `test.csv`) come from an annual test students across the country take between grades 3-8 in reading and math. The 198,426 data points we utilize here come specifically from the state of Oregon, and is simulated rather than actual data, although the distribution of this simulated data is very similar to that of the real data. This data can be accessed from our [GitHub](https://github.com/kgrove10/EDLD654_blog/tree/main/static/data) or from the [Kaggle competition](https://www.kaggle.com/c/edld-654-fall-2020/data) for this course. 

We also joined a couple additional data sets to this data that we think could help improve the predictive ability of our models by describing more about the schools in which individual students are enrolled. The first, `fallmembershipreport_20192020.xlsx` is a student enrollment report that provides information about the K-12 students who are enrolled on the first day of October of each year (this data is for the 2019-2020 school year). It can be accessed from the [Oregon Department of Education Website](https://www.oregon.gov/ode/reports-and-data/students/Pages/Student-Enrollment-Reports.aspx). The students included in this sample include all students enrolled in public schools and programs (including regular, alternative, charter, and other types of schools and programs). The report also includes students attending private schools if the student was placed there by a private entity or financed with public funds. No student is double-listed, that is, no student is enrolled in more than one school or district. Next, we join data from the [National Center for Educational Statistics (NCES)](https://nces.ed.gov/ccd/files.asp#Fiscal:2,LevelId:7,SchoolYearId:32,Page:1) which provides the number of students by school who are eligible for various free or reduced lunch programs. We will filter this data for just those students from Oregon. We join this data with [data on student counts](https://raw.githubusercontent.com/datalorax/ach-gap-variability/master/data/achievement-gaps-geocoded.csv).

Example code to read in and join this data is as follows. Because of the size of this data and in the interest of not exhausting the vector memory of this blogdown site, we will present example code and output below without actually evaluating it. Reproducible code and output for each of our models can be found on our [GitHub](https://github.com/kgrove10/EDLD654_blog/tree/main/static) under each of the three model folders. 

## Getting Started: Loading and Cleaning the Data

First, we will load some libraries into our environment that we will be using throughout this post. The classic {tidyverse} and for wrangling and {rio} for importing, our new {tidymodels} and the {skimr} package to help us explore our data and {knitr} to help us visualize tables.

```{r eval = FALSE}
library(tidyverse) #clean and tidy data
library(rio) # import data
library(tidymodels) # main modeling package
library(skimr) # create data visualizations
library(knitr) # create tables
``` 

Next, we're going to read in the data. When we are importing the data, we will also sightly clean it. For example, with the fall membership 2019-2020 student enrollment report, we will update the names of columns to be in snake case to both match the names of our variable names in our main, `train.csv` data (e.g. "Attending School ID" becomes "attnd_schl_inst_id"), and to make these variables easier to work with in R.  When importing our free and reduced lunch data, we clean names, and also replace missing data with 0s for student counts. Below, we will go over each of the variables present in our data. 

```{r eval = FALSE}
set.seed(500)
# import our training data, remove the classification column, since we are interested in score, not student classification
full_train <- read_csv("data/train.csv",
                       col_types = cols(.default = col_guess(), 
                                        calc_admn_cd = col_character()))  %>% 
  select(-classification)
  
#import our fall membership student enrollment report
sheets <- readxl::excel_sheets("data/fallmembershipreport_20192020.xlsx")
ode_schools <- readxl::read_xlsx("data/fallmembershipreport_20192020.xlsx", sheet = sheets[4])
str(ode_schools)

ethnicities <- ode_schools %>% select(attnd_schl_inst_id = `Attending School ID`,
                                      sch_name = `School Name`,
                                      contains("%")) %>%
  janitor::clean_names()

names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

#read in our free and reduced lunch data
frl <- rio::import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
                   setclass = "tbl_df")  %>%
  janitor::clean_names()  %>%
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>%
  mutate(student_count = replace_na(student_count, 0))  %>%
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>%
  janitor::clean_names()  %>%
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades, filter for oregon schools, 
# and change type of id to be numeric
stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv", setclass = "tbl_df")  %>%
  filter(state == "OR" & year == 1718)  %>%
  count(ncessch, wt = n)  %>%
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl <- left_join(frl, stu_counts)
```

Next, we need to join these three datasets together so we can use them to fit our tree models.  Before we do this, we need to make sure that the "id" we are going to join by is unique across all the datasets, meaning that it doesn't correspond to more than one row. The following function, called `unique_id()` from the work of [Thoen (2017)](https://edwinth.github.io/blog/unique_id/) will check if the variable we want to join our datasets by is a unique id. 

```{r eval = FALSE}
#function to check id is unique
unique_id <- function(x, ...) {
  id_set <- x %>% select(...)
  id_set_dist <- id_set %>% distinct
  if (nrow(id_set) == nrow(id_set_dist)) {
    TRUE
  } else {
    non_unique_ids <- id_set %>% 
      filter(id_set %>% duplicated()) %>% 
      distinct()
    suppressMessages(
      inner_join(non_unique_ids, x) %>% arrange(...)
    )
  }
}
```

When we run this function on our `ethnicities` data (from fall membership report), we see that the id variable `attnd_schl_inst_id` is not unique. To remedy this, without knowing more about how this data was collected, and which ethnicity values are correct, we will average across observations for each school to create one aggregate observation for each school that appears more than once. Specifically, we will group by school id, and average across the recorded ethnicity values for each ethnic group using the following code:

```{r eval = FALSE}
#make ethnicities have attnd_schl_inst_id as a unique identifier 
ethnicities <- ethnicities %>%
  group_by(attnd_schl_inst_id) %>%
  summarize(p_american_indian_alaska_native = mean(p_american_indian_alaska_native),
            p_asian = mean(p_asian),
            p_native_hawaiian_pacific_islander = mean(p_native_hawaiian_pacific_islander),
            p_black_african_american = mean(p_black_african_american),
            p_hispanic_latino = mean(p_hispanic_latino),
            p_white = mean(p_white),
            p_multiracial = mean(p_multiracial))

ethnicities %>% unique_id(attnd_schl_inst_id)
```

Now we can use the `unique_id()` function again, and it prints TRUE, indicating that attnd_schl_inst_id is now a unique identifier for our `ethnicities` data. 

Next, we can join all our data together: first, our `full_train` data from `train.csv` and our `ethnicities` data, and then our free and reduced lunch data, `frl`. Note that for both of these joins, we utilized left joins, in order to preserve the information in our left hand dataset, here, the data from `train.csv`, since our interest is to add additional data on ethnicities, student counts, and free and reduced lunch program data by school to each of our original observations. 

```{r eval = FALSE}
#Join ethnicity data and training data
full_train <- left_join(full_train, ethnicities)


# add frl data to train data
frl_fulltrain <- left_join(full_train, frl)
```

Now our three datasets are all loaded, cleaned, and joined together to make one cohesive data set we will use for all three of our tree-based models. Before we move on to preparing the data for modeling, let's explore and describe the variables present in our joined data set, `frl_fulltrain`. 

Each row of our data corresponds to an individual student observation. However, the data contained in each row includes both student-level variables, such as the student's grade, gender, ethnicity, and enrollment in various programs, as well as school-level and district-level data, such as school ethnicity proportions and the proportion of students who qualify for free or reduced lunch programs. A detailed data dictionary for the initial `train.csv` data can be found [here](), but we will attmpet to provide a brief description and overview of the 52 variables present in our `frl_fulltrain` data each of our three tree-based models will rely upon. 

Overall, our data consists of 17 numeric variables, 6 id variables, 1 date variable, and 28 categorical variables. 

Our 6 id variables are:

- `id` which identifies a single student, 
- `attnd_dist_inst_id` and `attnd_schl_inst_id` which represent the district and school, respectively where the student is enrolled
- `partic_dist_inst_id` and `partic_schl_inst_id` which represent the district and school, respectively, where the student was enrolled as of the first school day in May.
- `ncessch` which is the NCES school identifier

Our date variable, `tst_dt` gives the date the test was taken, in mm/dd/yyy format.

The summary statistics, including minimum and maximum, median, mean, and number of missing values are presented below for each of our 17 numeric variables, including our outcome variable, `score`, which is the student's overall test score. Our numeric predictor variables also the grade the student is enrolled in (`enrl_grd`), the school's latitude and longitude (`lat` and `lon`), the ethnic makeup of the school by proportion (`p_american_indian_alaska_native`, `p_asian`, `p_native_hawaiian_pacific_islander`, `p_black_african_american`, `p_hispanic_latino`, `p_white`, and `p_multiracial`), the proportion of students in the school eligible for free and reduced price lunch (`free_lunch_qualified` and `reduced_price_lunch_qualified`) as well as other data related to this free/reduced lunch data, including the number of observations missing or not applicable (`missing`, `not_applicable`), and `no_category_codes` representing categories not specified in the subtotal/total, and the student count across grades for each school, `n`. 

```{r, echo = FALSE, message = FALSE, results = "asis"} 
library(rio) 
library(here)
library(qwraps2)
options(qwraps2_markup = "markdown")

datadesc <- import(here("static", "datadescriptives.csv"))

summary_table(datadesc) 
```

Finally, our categorical variables include student specific measures:

- `gndr`, student gender, M = male, F = female
- `ethnic_cd`, student ethnicity (see [data dictionary]() for specific coding)
- 

## Overall Data Examination

Now that we've examined our individual variables in detail, we can use the {skimr} package's `skim()` function to take an overall look at the distribution of our numeric variables.  

```{r eval = FALSE}
frl_fulltrain %>% 
  select(-contains("id"), -ncessch) %>%  #remove id variables
  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% 
  modify_if(is.character, as.factor) %>%  # convert character variables to factors
  skim() %>% 
  select(-starts_with("numeric.p")) # remove quartiles
```

The skimr output, below, tells us that we have 17 numeric variables, with 8 of them having some missing data. We'll address this issue of missing data when we pre-process our recipe below. Most of the distributions of these numeric variables noticeably deviate from normality, but this isn't concerning for us, since the tree-based models we'll be using for our model fits are all non-parametric, meaning they make fewer assumptions about the underlying distribution of the data, and do not assume or require normality. 

```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/plots/Slide1.pdf"', alt = "Skimr Output")
```

```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/plots/Slide2.pdf"', alt = "Skimr Variable Relations")
```

We can also use {corrplot} to visualize relationships among our numeric variables. However, this is a limited visual given that most of our predictors are acutally categorical. We do see strong correlations between several of our predictors, which isn't too concerning in terms of prediction, since tree-based models aren't particularly vulnerable to multi-collinearity, but is an issue with interpreting these models, since they could be relying on a variable to make splits that not the important or impactful variable you are actually interested in, but just a variable highly correlated with it (ensemble models like random forest help to address this issue - see the [random forest](https://edld654finalproject.netlify.app/2020/12/02/random-forest/) post for more discussion of this issue). 

```{r eval = FALSE}
#look at relations between variables
frl_fulltrain %>% 
  select(-contains("id"), -ncessch) %>% 
  select_if(is.numeric) %>% 
  select(score, everything()) %>% 
  cor(use = "complete.obs") %>% 
  corrplot::corrplot()
```

```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/plots/variablerelations.pdf"', alt = "Variable Relations")
```


## Splitting, Resampling, and Pre-Processing the Data 

Now that we've examined our data, the first step of preparing to fit our tree models is splitting the data (`frl_fulltrain`) into two separate sets:

(1) The `training` set, which is used to train a model and tune the model's hyperparameters - essentially to fit the model to a (hopefully representative) subset of the data, and alter the parameters of the model that we set before the model starts fitting to the data. Hyperparameters are discussed more in each of the model-specific posts. 

(2) The `testing` set, which is used to evaluate the performance of the model we've trained, sometimes called "out-of-sample" accuracy. It also helps us avoid overfitting our model to the training dataset, or fitting a model too closely to the data we use to train, such that it is unable to accurately describe additional data outside of the training set because the model is describing random fluctuations ("noise") that isn't actually meaningful in our training set. This is the machine learning equivalent of getting your clothes tailored - they'll fit you *perfectly*, but even someone who wears the same standard clothing size you do would find these specially-tailored-to-you clothes ill-fitting for them.  In this analogy, our goal is to figure out the clothing size of the data we're training on and create clothing (a model) that our data can wear, but that the other data we haven't seen (such as the test set) will be able to wear too (generalizability).

You might be thinking at this point - wait, we already have two data files called `train.csv` and `test.csv`... are we using them, or are we splitting again?  The answer is we're splitting again. We don't touch the `test.csv` file at all until we have fit a final model we're happy with using our `train.csv` file (training on the `test.csv` is data leakage, and would give us an inaccurate measure of our model's performance, since we'd have fit it to the data we're supposed to use to tell us how well it performs). So instead, we need to find a way to both train and test within our `train.csv` data (now called `frl_fulltrain` after we added in the additional data above), so we split *again* to create a sub-training and testing set from the original training data. 

To split or `frl_fulltrain` data, we will use the fuction `initial_split()` from the {rsample} package. By default, this function splits the data at random such that 75% of the data is in the training set and 25% in the testing set. This can be altered using the argument `prop = `, although in this case, we're going to stick with this default split proportion. 

We're then going to do something called "resampling" on our newly created training set `train` (made up of 75% of the `frl_fulltrain` data). We carry out resampling because an important part of training our model is seeing how well it's doing...  but we can't touch our training set, since that would be data leakage. So since we can't use that data, we run into the issue of re-predicting the same data over and over, which isn't ideal, since it biases results by increasing the probability of overfitting, and means we won't have any measure of variance with only one performance measure.  Instead, we split the data into smaller re-samples that have an "analysis" and a "assessment" set, so we can train on the "analysis" set and evaluate the model we create on the "assessment" set without using our training set. 

There are many ways to do this resampling, including k-fold cross validation, Monte Carlo cross-validation, bootstrapping, and leave-one-out cross validation. We're going to use the most common resampling method in machine learning, k-fold cross validation. in this resampling method, we randomly split the data into some number (k) of distinct samples, or "folds" of approximately equal size. Here, we are using 10-fold cross validation, meaning we split our `train` data into 10 distinct folds. Each fold contains a distinct 10% of our `train` data, meaning that each row appears in only one of our assessment samples, and the other 90% of the `train` data serves as the analysis set for that fold. If we had done 6-fold cross validation, each fold would have approximately 16.66% of our data as the assessment set, and the remaining 83.33% would serve as the analysis set. 

Here, we conduct this k-fold cross validation using the function `vfold_cv` where we set which variable to stratify on, `strata =` (here, our dv, "score") and the value of k, or the number of folds we want to split our data into, using `v = `). 

Note that we also use the function `set.seed()`. Because our data is randomly split, we use this function to set the starting number used to generate a sequence of random numbers, which just ensures that everytime we run this code, we will get the same sample and results. Without this, the randomization would mean our results would differ slightly each time, which can be frustrating when trying to collaborate and write up results!

```{r eval=FALSE}
set.seed(500)
edu_split <- initial_split(frl_fulltrain)
train <- training(edu_split)
test <- testing(edu_split)

train_cv <- vfold_cv(train, strata = "score", v = 10)
```

#Preparing and baking the recipe (i.e. Feature engineering)

Now that we've split our data, and resampled it, we need to prepare it for model fitting, using a process called "pre-processing". Before we add in our data to the model, we are going to set up an object that pre-processes our data. This is called a recipe. To create a recipe, we will first specify a formula for our model, indicating which variable is our outcome and which are our predictors. Here, we will use all variables other than score as predictors. We then specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role, encode, omit or impute missing data, and/or model other feature engineering steps . 

Our recipe does the following:

* Sets score as the outcome, and models all other variables in the dataset
* Transforms dates into numberic variables
* Removes variables that contain "bnch"
* Sets the role of ID variables (including ncessch) 
* Imputes missing data
* Removes zero-variance and near-zero variance predictor variables 
* Dummy codes all nominal predictor variables

Note: The order of these operations matters. If you end up with a rank-deficient model you need to revise your recipe. 

A complete list of possible pre-processing steps can be found [here](https://www.tidymodels.org/find/recipes/)

```{r eval=FALSE}
rec <- recipe(score ~., train) %>%
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
              time_index = as.numeric(tst_dt)) %>%
  step_rm(contains("bnch")) %>% 
  update_role(tst_dt, new_role = "time_index") %>%
  update_role(contains("id"), ncessch, new_role = "id") %>% #removed sch_name
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal(), -all_outcomes()) %>%
  step_rollimpute(all_numeric(), -all_outcomes(), -has_role("id"), -n) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id")) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
  step_dummy(all_nominal(), -has_role(match = "id"), -all_outcomes(), -time_index) %>%
  step_nzv(all_predictors()) %>%
  step_interact(terms = ~ starts_with('lat'):starts_with("lon"))

prep(rec) 
```

## Setting up a model and workflow

There are a few core elements that you will need to specify to create a model. This apllies to all models. 

* The type of model: Here we will be focusing on tree-based models. 
* The engine: `set_engine()` calls the package to support the model you specified.
* The mode: `set_mode()` indicates the type of prediction you would like to use in your model. You choose between regression and classification. In our case, we will be choosing regression because we are looking to predict student scores, which is a continuous predictor.
* The arguments: `set_args()` allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered.

The final step here is to creat a workflow. A workflow combines all the elements we set up to create a cohesive framework, called a workflow, so we can run our desired models following the same exact framework. 