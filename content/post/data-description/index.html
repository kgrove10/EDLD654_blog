---
title: Intro & Data Description
author: 'Alexis Adams-Clark, Kivalina Grove, and Ouafaa Hmaddi'
date: '2020-11-30'
slug: data-description
categories: []
tags: []
---

<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<p>In this blog, we are going to describe how we used machine learning with the <a href="https://www.tidymodels.org/">tidymodels</a> package (and a couple other packages we’ll introduce later) to fit three different tree-based models to predict student test scores (bagged trees, random forests, and boosted trees).</p>
<p>Before we get to fitting our models though, we’ll introduce our data and the preprocessing and feature engineering steps we took to prepare our data to fit these three models.</p>
<div id="intro-to-the-data" class="section level2">
<h2>Intro to the Data</h2>
<p>The main data we are using for this project, (<code>train.csv</code> and <code>test.csv</code>) come from an annual test students across the country take between grades 3-8 in reading and math. The 189,426 data points we utilize here come specifically from the state of Oregon, and is simulated rather than actual data, although the distribution of this simulated data is very similar to that of the real data. This data can be accessed from our <a href="https://github.com/kgrove10/EDLD654_blog/tree/main/static/data">GitHub</a> or from the <a href="https://www.kaggle.com/c/edld-654-fall-2020/data">Kaggle competition</a> for this course.</p>
<p>We also joined a couple additional data sets to this data that we think could help improve the predictive ability of our models by describing more about the schools in which individual students are enrolled. The first, <code>fallmembershipreport_20192020.xlsx</code> is a student enrollment report that provides information about the K-12 students who are enrolled on the first day of October of each year (this data is for the 2019-2020 school year). It can be accessed from the <a href="https://www.oregon.gov/ode/reports-and-data/students/Pages/Student-Enrollment-Reports.aspx">Oregon Department of Education Website</a>. The students included in this sample include all students enrolled in public schools and programs (including regular, alternative, charter, and other types of schools and programs). The report also includes students attending private schools if the student was placed there by a private entity or financed with public funds. No student is double-listed, that is, no student is enrolled in more than one school or district. Next, we join data from the <a href="https://nces.ed.gov/ccd/files.asp#Fiscal:2,LevelId:7,SchoolYearId:32,Page:1">National Center for Educational Statistics (NCES)</a> which provides the number of students by school who are eligible for various free or reduced lunch programs. We will filter this data for just those students from Oregon. We join this data with <a href="https://raw.githubusercontent.com/datalorax/ach-gap-variability/master/data/achievement-gaps-geocoded.csv">data on student counts</a>.</p>
<p>Example code to read in and join this data is as follows. Because of the size of this data and in the interest of not exhausting the vector memory of this blogdown site, we will present example code and output below without actually evaluating it. Reproducible code and output for each of our models can be found on our <a href="https://github.com/kgrove10/EDLD654_blog/tree/main/static">GitHub</a> under each of the three model folders.</p>
</div>
<div id="getting-started-loading-and-cleaning-the-data" class="section level2">
<h2>Getting Started: Loading and Cleaning the Data</h2>
<p>First, we will load some libraries into our environment that we will be using throughout this post. The classic {tidyverse} and for wrangling and {rio} for importing, our new {tidymodels} and the {skimr} package to help us explore our data and {knitr} to help us visualize tables.</p>
<pre class="r"><code>library(tidyverse) #clean and tidy data
library(rio) # import data
library(tidymodels) # main modeling package
library(skimr) # create data visualizations
library(knitr) # create tables</code></pre>
<p>Next, we’re going to read in the data. When we are importing the data, we will also sightly clean it. For example, with the fall membership 2019-2020 student enrollment report, we will update the names of columns to be in snake case to both match the names of our variable names in our main, <code>train.csv</code> data (e.g. “Attending School ID” becomes “attnd_schl_inst_id”), and to make these variables easier to work with in R. When importing our free and reduced lunch data, we clean names, and also replace missing data with 0s for student counts. Below, we will go over each of the variables present in our data.</p>
<pre class="r"><code>set.seed(500)
# import our training data, remove the classification column, since we are interested in score, not student classification
full_train &lt;- read_csv(&quot;data/train.csv&quot;,
                       col_types = cols(.default = col_guess(), 
                                        calc_admn_cd = col_character()))  %&gt;% 
  select(-classification)
  
#import our fall membership student enrollment report
sheets &lt;- readxl::excel_sheets(&quot;data/fallmembershipreport_20192020.xlsx&quot;)
ode_schools &lt;- readxl::read_xlsx(&quot;data/fallmembershipreport_20192020.xlsx&quot;, sheet = sheets[4])
str(ode_schools)

ethnicities &lt;- ode_schools %&gt;% select(attnd_schl_inst_id = `Attending School ID`,
                                      sch_name = `School Name`,
                                      contains(&quot;%&quot;)) %&gt;%
  janitor::clean_names()

names(ethnicities) &lt;- gsub(&quot;x2019_20_percent&quot;, &quot;p&quot;, names(ethnicities))

#read in our free and reduced lunch data
frl &lt;- rio::import(&quot;https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip&quot;,
                   setclass = &quot;tbl_df&quot;)  %&gt;%
  janitor::clean_names()  %&gt;%
  filter(st == &quot;OR&quot;)  %&gt;%
  select(ncessch, lunch_program, student_count)  %&gt;%
  mutate(student_count = replace_na(student_count, 0))  %&gt;%
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %&gt;%
  janitor::clean_names()  %&gt;%
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades, filter for oregon schools, 
# and change type of id to be numeric
stu_counts &lt;- import(&quot;https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv&quot;, setclass = &quot;tbl_df&quot;)  %&gt;%
  filter(state == &quot;OR&quot; &amp; year == 1718)  %&gt;%
  count(ncessch, wt = n)  %&gt;%
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl &lt;- left_join(frl, stu_counts)</code></pre>
<p>Next, we need to join these three datasets together so we can use them to fit our tree models. Before we do this, we need to make sure that the “id” we are going to join by is unique across all the datasets, meaning that it doesn’t correspond to more than one row. The following function, called <code>unique_id()</code> from the work of <a href="https://edwinth.github.io/blog/unique_id/">Thoen (2017)</a> will check if the variable we want to join our datasets by is a unique id.</p>
<pre class="r"><code>#function to check id is unique
unique_id &lt;- function(x, ...) {
  id_set &lt;- x %&gt;% select(...)
  id_set_dist &lt;- id_set %&gt;% distinct
  if (nrow(id_set) == nrow(id_set_dist)) {
    TRUE
  } else {
    non_unique_ids &lt;- id_set %&gt;% 
      filter(id_set %&gt;% duplicated()) %&gt;% 
      distinct()
    suppressMessages(
      inner_join(non_unique_ids, x) %&gt;% arrange(...)
    )
  }
}</code></pre>
<p>When we run this function on our <code>ethnicities</code> data (from fall membership report), we see that the id variable <code>attnd_schl_inst_id</code> is not unique. To remedy this, without knowing more about how this data was collected, and which ethnicity values are correct, we will average across observations for each school to create one aggregate observation for each school that appears more than once. Specifically, we will group by school id, and average across the recorded ethnicity values for each ethnic group using the following code:</p>
<pre class="r"><code>#make ethnicities have attnd_schl_inst_id as a unique identifier 
ethnicities &lt;- ethnicities %&gt;%
  group_by(attnd_schl_inst_id) %&gt;%
  summarize(p_american_indian_alaska_native = mean(p_american_indian_alaska_native),
            p_asian = mean(p_asian),
            p_native_hawaiian_pacific_islander = mean(p_native_hawaiian_pacific_islander),
            p_black_african_american = mean(p_black_african_american),
            p_hispanic_latino = mean(p_hispanic_latino),
            p_white = mean(p_white),
            p_multiracial = mean(p_multiracial))

ethnicities %&gt;% unique_id(attnd_schl_inst_id)</code></pre>
<p>Now we can use the <code>unique_id()</code> function again, and it prints TRUE, indicating that attnd_schl_inst_id is now a unique identifier for our <code>ethnicities</code> data.</p>
<p>Next, we can join all our data together: first, our <code>full_train</code> data from <code>train.csv</code> and our <code>ethnicities</code> data, and then our free and reduced lunch data, <code>frl</code>. Note that for both of these joins, we utilized left joins, in order to preserve the information in our left hand dataset, here, the data from <code>train.csv</code>, since our interest is to add additional data on ethnicities, student counts, and free and reduced lunch program data by school to each of our original observations.</p>
<pre class="r"><code>#Join ethnicity data and training data
full_train &lt;- left_join(full_train, ethnicities)


# add frl data to train data
frl_fulltrain &lt;- left_join(full_train, frl)</code></pre>
<p>Now our three datasets are all loaded, cleaned, and joined together to make one cohesive data set we will use for all three of our tree-based models. Before we move on to preparing the data for modeling, let’s explore and describe the variables present in our joined data set, <code>frl_fulltrain</code>. The <a href="https://github.com/kgrove10/EDLD654_blog/blob/main/static/data/data_dictionary.csv">data dictionary</a> for the main data (<code>test.csv</code> and <code>train.csv</code>) is also available on our github.</p>
<p>Each row of our data corresponds to an individual student observation. However, the data contained in each row includes both student-level variables, such as the student’s grade, gender, ethnicity, and enrollment in various programs, as well as school-level and district-level data, such as school ethnicity proportions and the proportion of students who qualify for free or reduced lunch programs. A detailed data dictionary for the initial <code>train.csv</code> data can be found <a href="">here</a>, but we will attempt to provide a brief description and overview of the 52 variables present in our <code>frl_fulltrain</code> data each of our three tree-based models will rely upon.</p>
<p>Overall, our data consists of 17 numeric variables, 6 id variables, 1 date variable, and 28 categorical variables.</p>
<p>Our 6 id variables are:</p>
<ul>
<li><code>id</code> which identifies a single student,</li>
<li><code>attnd_dist_inst_id</code> and <code>attnd_schl_inst_id</code> which represent the district and school, respectively where the student is enrolled</li>
<li><code>partic_dist_inst_id</code> and <code>partic_schl_inst_id</code> which represent the district and school, respectively, where the student was enrolled as of the first school day in May.</li>
<li><code>ncessch</code> which is the NCES school identifier</li>
</ul>
<p>Our date variable, <code>tst_dt</code> gives the date the test was taken, in mm/dd/yyy format.</p>
<p>The summary statistics, including minimum and maximum, median, mean, and number of missing values are presented below for each of our 17 numeric variables, including our outcome variable, <code>score</code>, which is the student’s overall test score. Our numeric predictor variables also the grade the student is enrolled in (<code>enrl_grd</code>), the school’s latitude and longitude (<code>lat</code> and <code>lon</code>), the ethnic makeup of the school by proportion (<code>p_american_indian_alaska_native</code>, <code>p_asian</code>, <code>p_native_hawaiian_pacific_islander</code>, <code>p_black_african_american</code>, <code>p_hispanic_latino</code>, <code>p_white</code>, and <code>p_multiracial</code>), the proportion of students in the school eligible for free and reduced price lunch (<code>free_lunch_qualified</code> and <code>reduced_price_lunch_qualified</code>) as well as other data related to this free/reduced lunch data, including the number of observations missing or not applicable (<code>missing</code>, <code>not_applicable</code>), and <code>no_category_codes</code> representing categories not specified in the subtotal/total, and the student count across grades for each school, <code>n</code>.</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">datadesc (N = 189,426)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>enrl_grd</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">5.00 (4.00, 7.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">5.46 ± 1.69</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left"><strong>score</strong></td>
<td align="left">  </td>
</tr>
<tr class="odd">
<td align="left">   minimum</td>
<td align="left">1,601</td>
</tr>
<tr class="even">
<td align="left">   median (IQR)</td>
<td align="left">2,498.00 (2,421.00, 2,576.00)</td>
</tr>
<tr class="odd">
<td align="left">   mean (sd)</td>
<td align="left">2,498.99 ± 115.83</td>
</tr>
<tr class="even">
<td align="left">   maximum</td>
<td align="left">3,550</td>
</tr>
<tr class="odd">
<td align="left"><strong>lat</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">42.01</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">45.25 (44.17, 45.50)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">44.77 ± 1.02</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">46.18</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,400/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>lon</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">-124.50</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">-122.82 (-123.04, -122.54)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">-122.52 ± 1.19</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">-116.94</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,400/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_american_indian_alaska_native</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.01 (0.00, 0.01)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.01 ± 0.04</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0.88</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_asian</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.01 (0.01, 0.04)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.04 ± 0.07</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0.62</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_native_hawaiian_pacific_islander</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.00 (0.00, 0.01)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.01 ± 0.01</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0.33</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_black_african_american</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.01 (0.00, 0.02)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.02 ± 0.04</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0.50</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_hispanic_latino</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.18 (0.10, 0.33)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.24 ± 0.19</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">1.00</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_white</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.65 (0.45, 0.77)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.61 ± 0.20</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">1.00</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>p_multiracial</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0.00</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.07 (0.04, 0.08)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.07 ± 0.03</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0.35</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">154/189,426 (0)</td>
</tr>
<tr class="odd">
<td align="left"><strong>free_lunch_qualified</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">210.00 (122.00, 306.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">227.73 ± 146.13</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">813</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,216/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>reduced_price_lunch_qualified</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">36.00 (22.00, 52.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">39.51 ± 24.44</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">132</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,216/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>missing</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.00 (0.00, 0.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.00 ± 0.00</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,216/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>not_applicable</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">0.00 (0.00, 0.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">0.00 ± 0.00</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,216/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>no_category_codes</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">245.00 (152.00, 356.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">267.24 ± 164.39</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">920</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,216/189,426 (2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>n</strong></td>
<td align="left">  </td>
</tr>
<tr class="even">
<td align="left">   minimum</td>
<td align="left">12</td>
</tr>
<tr class="odd">
<td align="left">   median (IQR)</td>
<td align="left">597.00 (420.00, 1,157.00)</td>
</tr>
<tr class="even">
<td align="left">   mean (sd)</td>
<td align="left">806.08 ± 540.65</td>
</tr>
<tr class="odd">
<td align="left">   maximum</td>
<td align="left">3,144</td>
</tr>
<tr class="even">
<td align="left">   Unknown</td>
<td align="left">3,216/189,426 (2)</td>
</tr>
</tbody>
</table>
<p>Finally, our categorical variables are:</p>
<ul>
<li><code>gndr</code>, student gender, M = male, F = female</li>
<li><code>ethnic_cd</code>, student ethnicity (see the <a href="https://github.com/kgrove10/EDLD654_blog/blob/main/static/data/data_dictionary.csv">data dictionary</a> for specific coding)</li>
<li><code>calc_admn_cd</code>, code representing any special circumstances impacting test administration</li>
<li><code>tst_bnch</code>, code indicating the benchmark level of the test administered</li>
<li><code>migrant_ed_fg</code>, student participation in program for migratory children</li>
<li><code>ind_ed_fg</code>, student participation in program for American Indian children</li>
<li><code>sp_ed_fg</code>, student participation in an IEP program</li>
<li><code>tag_ed_fg</code>, student participation in a talented and gifted program</li>
<li><code>econ_dsvtg</code>, student eligibility in a free/reduced lunch program</li>
<li><code>ayp_lep</code>, student eligible to receive limited English proficient program services</li>
<li><code>stay_in_dist</code>, student enrolled for over half of the days in the school year in their resident district as of May</li>
<li><code>stay_in_schl</code>, student enrolled for over half of the days in the school year in their resident school as of May</li>
<li><code>dist_sped</code>, student enrolled in district special education program</li>
<li><code>trgt_assist_fg</code>, student record included in Title 1 Targeted Assistance for annual school performance calculations</li>
<li><code>ayp_dist_partic</code>, student record included in AYP (adequate yearly progress) district participation calculations</li>
<li><code>ayp_dist_prfm</code>, student record included in AYP district performance calculations</li>
<li><code>ayp_schl_partic</code>, student record included in AYP school participation calculations</li>
<li><code>ayp_schl_prfm</code>, student record included in AYP school performance calculations</li>
<li><code>rc_dist_partic</code>, student record included in report card district participation calculations</li>
<li><code>rc_dist_prfm</code>, student record included in report card district performance calculations</li>
<li><code>rc_schl_partic</code>, student record included in report card school participation calculations</li>
<li><code>rc_schl_prfm</code>, student record included in report card school performance calculations</li>
<li><code>lang_cd</code>, code indicating language of test (S = Spanish, blank = English)</li>
<li><code>tst_atmpt_fg</code>, code describing whether the test was attempted (yes or partially)</li>
<li><code>grp_rpp_dist_partic</code>, record included in group report district participation calculations</li>
<li><code>grp_rpt_dist_prfm</code>, record included in group report district performance calculations</li>
<li><code>grp_rpp_schl_partic</code>, record included in group report school participation calculations</li>
<li><code>grp_rpt_schl_prfm</code>, record included in group report school performance calculations</li>
</ul>
</div>
<div id="overall-data-examination" class="section level2">
<h2>Overall Data Examination</h2>
<p>Now that we’ve examined our individual variables in detail, we can use the {skimr} package’s <code>skim()</code> function to take an overall look at the distribution of our numeric variables.</p>
<pre class="r"><code>frl_fulltrain %&gt;% 
  select(-contains(&quot;id&quot;), -ncessch) %&gt;%  #remove id variables
  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %&gt;% 
  modify_if(is.character, as.factor) %&gt;%  # convert character variables to factors
  skim() %&gt;% 
  select(-starts_with(&quot;numeric.p&quot;)) # remove quartiles</code></pre>
<p>The skimr output, below, tells us that we have 17 numeric variables, with 8 of them having some missing data. We’ll address this issue of missing data when we pre-process our recipe below. Most of the distributions of these numeric variables noticeably deviate from normality, but this isn’t concerning for us, since the tree-based models we’ll be using for our model fits are all non-parametric, meaning they make fewer assumptions about the underlying distribution of the data, and do not assume or require normality.</p>
{{% figure src="/plots/Slide1.pdf" alt="Skimr Output" %}}
{{% figure src="/plots/Slide2.pdf" alt="Skimr Variable Relations" %}}
<p>We can also use {corrplot} to visualize relationships among our numeric variables. However, this is a limited visual given that most of our predictors are actually categorical. We do see strong correlations between several of our predictors, which isn’t too concerning in terms of prediction, since tree-based models aren’t particularly vulnerable to multi-collinearity, but is an issue with interpreting these models, since they could be relying on a variable to make splits that not the important or impactful variable you are actually interested in, but just a variable highly correlated with it (ensemble models like random forest help to address this issue - see the <a href="https://edld654finalproject.netlify.app/2020/12/02/random-forest/">random forest</a> post for more discussion of this issue).</p>
<pre class="r"><code>#look at relations between variables
frl_fulltrain %&gt;% 
  select(-contains(&quot;id&quot;), -ncessch) %&gt;% 
  select_if(is.numeric) %&gt;% 
  select(score, everything()) %&gt;% 
  cor(use = &quot;complete.obs&quot;) %&gt;% 
  corrplot::corrplot()</code></pre>
{{% figure src="/plots/variablerelations.pdf" alt="Variable Relations" %}}
</div>
<div id="splitting-resampling-and-pre-processing-the-data" class="section level2">
<h2>Splitting, Resampling, and Pre-Processing the Data</h2>
<p>Now that we’ve examined our data, the first step of preparing to fit our tree models is splitting the data (<code>frl_fulltrain</code>) into two separate sets:</p>
<ol style="list-style-type: decimal">
<li><p>The <code>training</code> set, which is used to train a model and tune the model’s hyperparameters - essentially to fit the model to a (hopefully representative) subset of the data, and alter the parameters of the model that we set before the model starts fitting to the data. Hyperparameters are discussed more in each of the model-specific posts.</p></li>
<li><p>The <code>testing</code> set, which is used to evaluate the performance of the model we’ve trained, sometimes called “out-of-sample” accuracy. It also helps us avoid overfitting our model to the training dataset, or fitting a model too closely to the data we use to train, such that it is unable to accurately describe additional data outside of the training set because the model is describing random fluctuations (“noise”) that isn’t actually meaningful in our training set. This is the machine learning equivalent of getting your clothes tailored - they’ll fit you <em>perfectly</em>, but even someone who wears the same standard clothing size you do would find these specially-tailored-to-you clothes ill-fitting for them. In this analogy, our goal is to figure out the clothing size of the data we’re training on and create clothing (a model) that our data can wear, but that the other data we haven’t seen (such as the test set) will be able to wear too (generalizability).</p></li>
</ol>
<p>You might be thinking at this point - wait, we already have two data files called <code>train.csv</code> and <code>test.csv</code>… are we using them, or are we splitting again? The answer is we’re splitting again. We don’t touch the <code>test.csv</code> file at all until we have fit a final model we’re happy with using our <code>train.csv</code> file (training on the <code>test.csv</code> is data leakage, and would give us an inaccurate measure of our model’s performance, since we’d have fit it to the data we’re supposed to use to tell us how well it performs). So instead, we need to find a way to both train and test within our <code>train.csv</code> data (now called <code>frl_fulltrain</code> after we added in the additional data above), so we split <em>again</em> to create a sub-training and testing set from the original training data.</p>
<p>To split or <code>frl_fulltrain</code> data, we will use the function <code>initial_split()</code> from the {rsample} package. By default, this function splits the data at random such that 75% of the data is in the training set and 25% in the testing set. This can be altered using the argument <code>prop =</code>, although in this case, we’re going to stick with this default split proportion.</p>
<p>We’re then going to do something called “resampling” on our newly created training set <code>train</code> (made up of 75% of the <code>frl_fulltrain</code> data). We carry out resampling because an important part of training our model is seeing how well it’s doing… but we can’t touch our training set, since that would be data leakage. So since we can’t use that data, we run into the issue of re-predicting the same data over and over, which isn’t ideal, since it biases results by increasing the probability of overfitting, and means we won’t have any measure of variance with only one performance measure. Instead, we split the data into smaller re-samples that have an “analysis” and a “assessment” set, so we can train on the “analysis” set and evaluate the model we create on the “assessment” set without using our training set.</p>
<p>There are many ways to do this resampling, including k-fold cross validation, Monte Carlo cross-validation, bootstrapping, and leave-one-out cross validation. We’re going to use the most common resampling method in machine learning, k-fold cross validation. in this resampling method, we randomly split the data into some number (k) of distinct samples, or “folds” of approximately equal size. Here, we are using 10-fold cross validation, meaning we split our <code>train</code> data into 10 distinct folds. Each fold contains a distinct 10% of our <code>train</code> data, meaning that each row appears in only one of our assessment samples, and the other 90% of the <code>train</code> data serves as the analysis set for that fold. If we had done 6-fold cross validation, each fold would have approximately 16.66% of our data as the assessment set, and the remaining 83.33% would serve as the analysis set.</p>
<p>Here, we conduct this k-fold cross validation using the function <code>vfold_cv</code> where we set which variable to stratify on, <code>strata =</code> (here, our dv, “score”) and the value of k, or the number of folds we want to split our data into, using <code>v =</code>).</p>
<p>Note that we also use the function <code>set.seed()</code>. Because our data is randomly split, we use this function to set the starting number used to generate a sequence of random numbers, which just ensures that everytime we run this code, we will get the same sample and results. Without this, the randomization would mean our results would differ slightly each time, which can be frustrating when trying to collaborate and write up results!</p>
<pre class="r"><code>set.seed(500)
edu_split &lt;- initial_split(frl_fulltrain)
train &lt;- training(edu_split)
test &lt;- testing(edu_split)

train_cv &lt;- vfold_cv(train, strata = &quot;score&quot;, v = 10)</code></pre>
</div>
<div id="pre-processing-recipes-missing-data-and-feature-engineering" class="section level2">
<h2>Pre-Processing: Recipes, Missing Data, and Feature Engineering</h2>
<p>Now that we’ve split our data, and resampled it, we need to prepare it for model fitting, using a process called “pre-processing”. We do this by setting up a recipe, which allows us to specify the formula for our model and engage in various preprocessing steps including feature engineering, handling missing data, and assigning roles to our variables. More on pre-processing steps <a href="https://www.tidymodels.org/find/recipes/">here</a>.</p>
<p>In this case, our recipe does the following:</p>
<ul>
<li>Sets score as the outcome, and models all other variables in the dataset, using <code>score ~ .</code></li>
<li>Transforms our date variable, <code>tst_dt</code> into a recognizable date variable, using {lubridate} and gives it the role of “time_index”</li>
<li>Removes variables that contain “bnch”, namely, <code>tst_bnch</code> which indicates the benchmark level of the administered test</li>
<li>Sets the role of the 6 ID variables we identified above, including ncessch, as “id”</li>
<li>Uses <code>step_novel()</code> and <code>step_unknown()</code> on our categorical variables to assign new variables in the data we supply to a new value, or missing data to the factor level “unknown” to handle categories we might have missed in our training data by nature of random assignment, and missing data in our categorical variables.</li>
<li>Imputes missing data using <code>step_medianimpute()</code> and <code>step_rollimpute()</code> which substitutes missing values of numeric variables with the median of those variables, or in the case of roll impute, by the measure of location within a moving window (so it’s better able to represent data that have a trend in them by using the median of the observations surrounding, rather than just the overall median)</li>
<li>Removes zero-variance and near-zero variance predictor variables (such as <code>missing</code> and <code>not_applicable</code>)</li>
<li>Dummy codes all nominal predictor variables</li>
<li>And finally, interacts the latitude and longitude variables (<code>lat</code> and <code>lon</code>)</li>
</ul>
<p>It’s important to note that the order in which we complete these operations is important, and if done incorrectly, can lead to a rank-deficient model.</p>
<pre class="r"><code>rec &lt;- recipe(score ~., train) %&gt;%
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
              time_index = as.numeric(tst_dt)) %&gt;% #set to be date variable
  step_rm(contains(&quot;bnch&quot;)) %&gt;% 
  update_role(tst_dt, new_role = &quot;time_index&quot;) %&gt;% 
  update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id&quot;) %&gt;% 
  step_novel(all_nominal(), -all_outcomes()) %&gt;%
  step_unknown(all_nominal(), -all_outcomes()) %&gt;%
  step_rollimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id&quot;), -n) %&gt;%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id&quot;)) %&gt;%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;%
  step_dummy(all_nominal(), -has_role(match = &quot;id&quot;), -all_outcomes(), -time_index) %&gt;%
  step_nzv(all_predictors()) %&gt;%
  step_interact(terms = ~ starts_with(&#39;lat&#39;):starts_with(&quot;lon&quot;))

prep(rec) </code></pre>
<p>When we prep our recipe, the output shows us exactly what our recipe is doing, and which variables the steps impact:</p>
<pre><code>Data Recipe

Inputs:

       role #variables
         id          6
    outcome          1
  predictor         44
 time_index          1

Training data contained 142070 data points and 142070 incomplete rows. 

Operations:

Variable mutation for tst_dt, time_index [trained]
Variables removed tst_bnch [trained]
Novel factor level assignment for gndr, ethnic_cd, calc_admn_cd, ... [trained]
Unknown factor level assignment for gndr, ethnic_cd, calc_admn_cd, ... [trained]
Rolling Imputation for enrl_grd, lat, ... [trained]
Median Imputation for enrl_grd, lat, ... [trained]
Sparse, unbalanced variable filter removed calc_admn_cd, missing, not_applicable [trained]
Dummy variables from gndr, ethnic_cd, migrant_ed_fg, ind_ed_fg, ... [trained]
Sparse, unbalanced variable filter removed 85 items [trained]
Interactions with lat:lon [trained]</code></pre>
</div>
<div id="preview-of-what-is-to-come-setting-up-a-model-and-workflow" class="section level2">
<h2>Preview of what is to come: Setting up a model and workflow</h2>
<p>Finally, we’re ready to create a model! This is discussed with more specificity for each model on their respective blog posts, but there are several common elemetns we will need to specify for each of our models.</p>
<ol style="list-style-type: decimal">
<li>The model type: in this blog, we focus on three tree models, namely <code>bag_tree()</code>, <code>random_forest()</code> and <code>boost_tree()</code>.</li>
<li>The engine: using <code>set_engine()</code> we specify which package or system will be used to fit the model</li>
<li>The mode: using <code>set_mode()</code> we specify if we would like classification or regression. In all of our models, we aim to predict student test scores, so we will use regression.</li>
<li>The arguments: using <code>set_args()</code>, we can modify the parameters, or arguments of a model specification. Hyperparameter tuning can be set within this argument, and is discussed in more detail within the model posts.</li>
</ol>
<p>We will also utilize workflows in our model fits. A workflow is an object that can bundle together pre-processing, modeling, and post-processing requests in one step, such as our recipe and model specification. More detail and examples of creating workflows are present in each of the three tree-specific blog posts.</p>
</div>
