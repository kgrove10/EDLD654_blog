---
title: Bagged Tree
author: Ouafaa Hmaddi
date: '2020-12-01'
slug: bagged-tree
categories: []
tags: []
---
In this post, we will outline how we fit a bagged tree model to our data. 

## Bagged Tree Model Description

**What does the Bagged Tree Model do?**

Bagged trees are a special case of random forests where there is no sampling of columns when each tree is built. A bagged tree approach creates multiple subsets of data from the training set where each of these subsets is used to train a given decision tree. The predictions from all the different subsets are averaged together, giving us a stronger prediction than one tree could do independently.

**Hyperparameters to be optimized**

Unlike random forrest, there is not a way to easily access the “out-of-the-box” (OOB) samples with **{baguette}**. Thankfully, we can tune the model similar to what we do with random forrest using the **{ranger}** package, and this will allow us to access the OOB samples. To fit a bagged tree with **{ranger}**, we set the `mtry` argument equal to the number of predictors in our data frame calculated by extracting the number of columns in the data and substracting one (to not account for the outcome).

Overall, we tune the number of `trees` and `min_n` since `m_try` is already set depending on the number of predictors. 

**Why did we select this model?**

We selected this model as one of our three fits because we wanted to explore several models that utilize decision trees. Bagged tree specifically leads to more stable model predictions as it reduces model variability. Thus, it is important to note here that bagging would not be recommended when the model already has low variance as it increases computational burdens without improving the model. For example, linear regression models will generally not change much in their model predictions when using bagging. Overall, the noisier our data is, the more bags (i.e. resamples) we add to reduce the noise.

To illustrate this, it is important to understand that bagging like boosting is an ensemble method, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. This is basically, like when you feel like you are not learning much by studying alone so you go to the library and look for a study group to learn faster with an "ensemble" of other students. 

**Bagged Tree Model Assumptions**

Similar to the the decision trees model and other decision-tree based models, the bagged tree model share the same assumptions. Because decision trees function to split the data, they have no probabilistic model, and therefore bagging does not need to make any assumptions about the underlying data distribution. 

**Model Performance Evaluation**

We will evaluate the performance of this model after training by applying it to our held out test split from the training data (also called the validation set). We will examine rmse (appropriately, the root mean squared error, the standard deviation of the residuals), rsq (R-squared, the proportion of variance in the dependent variable explained by the variables in our regression model), and huber-loss (a loss function that represents absolute error, and is less sensitive to outliers than the mse and rmse), but largely rely upon the rmse as our single evaluator of the performance of this and the other tree models we present in this blog in order to select our final model.

## Fitting the Bagged Tree Model

First, we set up our data as you can see in the data description and setup [here](https://edld654finalproject.netlify.app/2020/11/30/data-description/).

**Model Fitting Procedures** 

### Model Fiting

```{r eval=FALSE}
set.seed(500)

#Specify model

mod_bag <- bag_tree() %>%
  set_mode("regression") %>%
  set_engine("rpart", times = 10) # 10 bootstrap resamples

#Create workflow

wflow_bag <- workflow() %>% 
  add_recipe(rec) %>%
  add_model(mod_bag)
```

```{r eval=FALSE}
set.seed(500)

fit_bag <- fit_resamples(
  wflow_bag,
  vfold_cv(train),
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))

collect_metrics(fit_bag)
```


### Model Turning 

Next, we will tune the number of trees hyperparameter. We set min_n to 2. We will use a functions that pulls multiple RMSEs and we will graph them to figure out the optimnal number of bags needed for this data.

```{r eval=FALSE}

pull_auc <- function(b) {
  # specify model
  mod <- bag_tree() %>% 
    set_mode("regression") %>% 
    set_args(cost_complexity = 0, min_n = 2) %>% 
    set_engine("rpart", times = b)
  
  # fit model to full training dataset
  m <- fit_resamples(mod, rec, train_cv)
  
  # extract the AUC & add the $b$ value
  auc <- show_best(m, "rmse") 
  auc$b <- b
  
  # return the AUC data frame
  auc
}

# specify candidate b models
b <- seq(5, 200, 25)

# Fit models
library(future)
plan(multisession)

aucs <- map_df(b, pull_auc)

plan(sequential)
```

```{r eval=FALSE}
ggplot(aucs, aes(b, mean)) +
  geom_line() +
  geom_point()  
```

```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/plots/rmse_auc.png"', alt = "RMSE AUC")
```

We see from the plot above that setting $b$ to 175 would be the optiomal number of trees. 


### Bagged Trees Visualization 

The plot below shows the root nodes from a bagged tree made of 60 trees (6 folds x 10 resamples). As noted based on our RMSE, the optimal number of resamples (trees) is 175 but for time and computional eficiency purposes we only set it to 10 in this visualization.

```{r eval=FALSE}
# extract roots
bag_roots <-  function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(models = map(.extracts,
                  ~.x$model_df)) %>% 
  select(-.extracts) %>% 
  unnest(cols = c(models)) %>% 
  mutate(root = map_chr(model,
                     ~as.character(.x$fit$frame[1, 1]))) %>%
  select(root)  
}

# plot
bag_roots(fit_bag) %>% 
  ggplot(mapping = aes(x = fct_rev(fct_infreq(root)))) + 
  geom_bar() + 
  coord_flip() + 
  labs(x = "Root", y = "Count")

```

```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/plots/bagtree_root.png"', alt = "Root nodes")
```

**Model Fitting Results**

Finally, in order to get the results of our model with the tuned trees hyperparameter (175), we will apply this model to our full training dataset.

From the fitted model, we then extract just the metrics in order to compare the results of this bagged tree model with our random forest and boosted tree models in order to choose a final model to make student score predictions from the test data (test.csv) and to submit to kaggle.

The following table shows our model fitting metrics.

```{r echo = FALSE}
blogdown::shortcode("figure", src ='"/plots/metrics.png"', alt = "Metrics Table")
```

The following code shows our the final fit of the model to the testing data to make predictions. 

```{r eval=FALSE}
#make predictions on test.csv
full_test <- read_csv("data/test.csv",
                      col_types = cols(.default = col_guess(), 
                                       calc_admn_cd = col_character()))

#join with ethnicity data
full_test_eth <- left_join(full_test, ethnicities) #join with FRL dataset

#join with frl
full_test_FRL <- left_join(full_test_eth, frl)

#workflow
fit_workflow <- fit(wflow_bag, frl_fulltrain)

#use model to make predictions for test dataset
preds_final <- predict(fit_workflow, full_test_FRL) #use model to make predictions for test dataset

head(preds_final)

pred_frame <- tibble(Id = full_test_FRL$id, Predicted = preds_final$.pred)

#create prediction file
write_csv(pred_frame, "fit_bag.csv")
```











