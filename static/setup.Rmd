![There are so many types of trees](https://www.homestratosphere.com/wp-content/uploads/2019/07/Two-categories-of-trees.jpg)


We’re going to walk through the basics for getting off the ground with machine learning using primarily the {tidymodels} package. Specifically, we will demontrate the application of three different tree-based methods for predicting student test scores. For further information about the {tidymodels}, you can visit [this page](https://www.tidymodels.org/).

#Getting started 

First, we will load some libraries into your environment that we will be using throughout this post. The classic {tidyverse} and for wrangling and {rio} for importing, our new {tidymodels} and the {skimr} package to help us explore our data and a host of other packages that will be required to run our machine learning models. Finally, we will load {baguette}, which is our key package in this post since we will be focusing on bagged trees. Make sure you check out the other posts about the other cool tree-based models. 

```{r eval = FALSE}
library(tidyverse) # manipulating data
library(rio) #importing data
library(tidymodels) 
library(skimr) # data visualization
library(baguette) # bagged trees
library(knitr) #table visualization 
```


#Read in data

We will be using simulated data which approximates reading and math scores for 3rd-8th grade students in Oregon public schools. Please see [this page for more detials](LINK FOR BLOG POST HERE). For the purpose of demonstration, we’ll be sampling 1% of the data with `sample_frac()` to keep computer processing time manageable.

```{r}
set.seed(500)

# import data and perform initial cleaning:
# Note: the data is called 'train.csv', but as we highlighted in the text above, for processing efficiency, we will actually further split this into its own training and testing data

full_train <- read_csv("data/train.csv",
                       col_types = cols(.default = col_guess(), 
                                        calc_admn_cd = col_character()))  %>% 
  select(-classification) %>% 
  sample_frac(.001)  # sample 1% of the data to reduce run time

# import fall membership report data and clean 
sheets <- readxl::excel_sheets("data/fallmembershipreport_20192020.xlsx")
ode_schools <- readxl::read_xlsx(here::here("data/fallmembershipreport_20192020.xlsx"), sheet = sheets[4])

ethnicities <- ode_schools %>% select(attnd_schl_inst_id = `Attending School ID`, 
 sch_name = `School Name`, 
 contains("%")) %>%
  janitor::clean_names()

names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

```

```{r warning= FALSE}

#function to check id is unique
unique_id <- function(x, ...) {
  id_set <- x %>% select(...)
  id_set_dist <- id_set %>% distinct
  if (nrow(id_set) == nrow(id_set_dist)) {
    TRUE
  } else {
    non_unique_ids <- id_set %>% 
      filter(id_set %>% duplicated()) %>% 
      distinct()
    suppressMessages(
      inner_join(non_unique_ids, x) %>% arrange(...)
    )
  }
}

#make ethnicities have attnd_schl_inst_id as a unique identifier 
ethnicities <- ethnicities %>%
  group_by(attnd_schl_inst_id) %>%
  summarize(p_american_indian_alaska_native = mean(p_american_indian_alaska_native),
            p_asian = mean(p_asian),
            p_native_hawaiian_pacific_islander = mean(p_native_hawaiian_pacific_islander),
            p_black_african_american = mean(p_black_african_american),
            p_hispanic_latino = mean(p_hispanic_latino),
            p_white = mean(p_white),
            p_multiracial = mean(p_multiracial))

#ethnicities %>% unique_id(attnd_schl_inst_id)

#Join ethnicity data and training data
full_train <- left_join(full_train, ethnicities)

# import and tidy free and reduced lunch data 
frl <- rio::import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
                   setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades
stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv", setclass = "tbl_df")  %>% 
  filter(state == "OR" & year == 1718)  %>% 
  count(ncessch, wt = n)  %>% 
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl <- left_join(frl, stu_counts)

# add frl data to train data
frl_fulltrain <- left_join(full_train, frl)
```

Our three datasets are all loaded, cleaned and join together to make one cohesive data set to use for our tree-based modelling. After joining, the data contains both student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch, etc.), all of which will be included for each 3 of our {tidymodels} tree-based examples.

#Explore data

We will use the `skim()` function from {skimr} to take a closer look at the distribution of our variables. Many numeric predictors are non-normal (see histograms below), but this is not a problem since tree-based methods are robust to non-normality.

```{r}
frl_fulltrain %>% 
  select(-contains("id"), -ncessch) %>%  # remove ID and irrelevant variables
  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% # convert test date to date
  modify_if(is.character, as.factor) %>%  # convert character vars to factors
  skim() %>% 
  select(-starts_with("numeric.p")) # remove quartiles
```

We will also use {corrplot} to better visualize the relationships among the numeric variables shown in the table above. However, this is a limited visual given that most of our predictors are acutally categorical.

```{r}
#look at relations between variables
frl_fulltrain %>% 
  select(-contains("id"), -ncessch) %>% 
  select_if(is.numeric) %>% 
  select(score, everything()) %>% 
  cor(use = "complete.obs") %>% 
  corrplot::corrplot()
```
#Split the data 

First, we split the data into two separate sets: a “training” set and a “testing” set. The training set is used to train a model and, if desired, to adjust (“tune” in ML language) the model’s hyperparameters before evaluating its final performance on the test data. This helps us assess the “out of sample” accuracy and limit overfitting to the training set. Overfitting refers to a model that models the training data too well. It occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize. 

To split our data, we use the initial_split() function from the {rsample} package. The default is that 75% of the data goes to the training set and 25% to the test set, but this can be adjusted with the prop argument. 

To prevent overfitting, we resample the data using vfold_cv() which outputs k-fold cross-validated versions of the training data, where k is the number of times we resample. By setting k to 6 data sets, we get a better estimate of the model’s out-of-sample accuracy. This decreases bias from overfitting. And of course, we should always remember to set the seeds first.

```{r}
set.seed(500)
edu_split <- initial_split(frl_fulltrain)
train <- training(edu_split)
test <- testing(edu_split)

train_cv <- vfold_cv(train, strata = "score", v = 6)
```

#Preparing and baking the recipe (i.e. Feature engineering)

Before we add in our data to the model, we are going to set up an object that pre-processes our data. This is called a recipe. To create a recipe, we will first specify a formula for our model, indicating which variable is our outcome and which are our predictors. Here, we will use all variables other than score as predictors. We then specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role, encode, omit or impute missing data, and/or model other feature engineering steps . 

Our recipe does the following:

* Sets score as the outcome, and models all other variables in the dataset
* Transforms dates into numberic variables
* Removes variables that contain "bnch"
* Sets the role of ID variables (including ncessch) 
* Imputes missing data
* Removes zero-variance and near-zero variance predictor variables 
* Dummy codes all nominal predictor variables

Note: The order of these operations matters. If you end up with a rank-deficient model you need to revise your recipe. 

A complete list of possible pre-processing steps can be found [here](https://www.tidymodels.org/find/recipes/)

```{r}
rec <- recipe(score ~., train) %>%
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
              time_index = as.numeric(tst_dt)) %>%
  step_rm(contains("bnch")) %>%
  update_role(tst_dt, new_role = "time_index") %>%
  update_role(contains("id"), ncessch, new_role = "id") %>% #removed sch_name
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal(), -all_outcomes()) %>%
  step_rollimpute(all_numeric(), -all_outcomes(), -has_role("id"), -n) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id")) %>%
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
  step_dummy(all_nominal(), -has_role(match = "id"), -all_outcomes(), -time_index) %>%
  step_nzv(all_predictors()) %>%
  step_interact(terms = ~ starts_with('lat'):starts_with("lon"))

prep(rec) 
```

#Setting up a model and workflow

There are a few core elements that you will need to specify to create a model. This apllies to all models. 

* The type of model: Here we will be focusing on tree-based models. 
* The engine: `set_engine()` calls the package to support the model you specified.
* The mode: `set_mode()` indicates the type of prediction you would like to use in your model. You choose between regression and classification. In our case, we will be choosing regression because we are looking to predict student scores, which is a continuous predictor.
* The arguments: `set_args()` allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered.

The final step here is to creat a workflow. A workflow combines all the elements we set up to create a cohesive framework, called a workflow, so we can run our desired models following the same exact framework. 