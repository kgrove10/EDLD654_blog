
R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #read in data
> library(tidyverse)
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──
✔ ggplot2 3.3.2     ✔ purrr   0.3.4
✔ tibble  3.0.4     ✔ dplyr   1.0.2
✔ tidyr   1.1.2     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
> full_train <- read_csv("data/train.csv",
+                        col_types = cols(.default = col_guess(), 
+                                         calc_admn_cd = col_character()))  %>% 
+   select(-classification)
> 
> #str(full_train)
> 
> set.seed(500)
> # import fall membership report data and clean 
> 
> sheets <- readxl::excel_sheets("data/fallmembershipreport_20192020.xlsx")
> ode_schools <- readxl::read_xlsx("data/fallmembershipreport_20192020.xlsx", sheet = sheets[4])
> str(ode_schools)
tibble [1,459 × 33] (S3: tbl_df/tbl/data.frame)
 $ Attending District Institution ID          : num [1:1459] 2063 2113 2113 1899 2252 ...
 $ District Name                              : chr [1:1459] "Adel SD 21" "Adrian SD 61" "Adrian SD 61" "Alsea SD 7J" ...
 $ Attending School ID                        : num [1:1459] 498 707 708 17 1208 ...
 $ School Name                                : chr [1:1459] "Adel Elementary School" "Adrian Elementary School" "Adrian High School" "Alsea Charter School" ...
 $ 2018-19 Total Enrollment                   : chr [1:1459] "8" "206" "89" "226" ...
 $ 2019-20 Total Enrollment                   : chr [1:1459] "7" "203" "89" "321" ...
 $ 2019-20 American Indian/Alaska Native      : chr [1:1459] "2" "0" "1" "5" ...
 $ 2019-20 % American Indian/Alaska Native    : num [1:1459] 0.2857 0 0.0112 0.0156 0.024 ...
 $ 2019-20 Asian                              : chr [1:1459] "0" "0" "1" "18" ...
 $ 2019-20 % Asian                            : num [1:1459] 0 0 0.0112 0.0561 0 ...
 $ 2019-20 Native Hawaiian/ Pacific Islander  : chr [1:1459] "0" "0" "0" "0" ...
 $ 2019-20 % Native Hawaiian/ Pacific Islander: num [1:1459] 0 0 0 0 0 0 0 0 0 0 ...
 $ 2019-20 Black/African American             : chr [1:1459] "0" "0" "0" "2" ...
 $ 2019-20 % Black/African American           : num [1:1459] 0 0 0 0.00623 0.00601 0 0 0.0303 0 0 ...
 $ 2019-20 Hispanic/ Latino                   : chr [1:1459] "0" "45" "15" "28" ...
 $ 2019-20 % Hispanic/ Latino                 : num [1:1459] 0 0.2217 0.1685 0.0872 0.1441 ...
 $ 2019-20 White                              : chr [1:1459] "5" "157" "70" "253" ...
 $ 2019-20 % White                            : num [1:1459] 0.714 0.773 0.787 0.788 0.766 ...
 $ 2019-20 Multiracial                        : chr [1:1459] "0" "1" "2" "15" ...
 $ 2019-20 % Multiracial                      : num [1:1459] 0 0.00493 0.02247 0.04673 0.06006 ...
 $ 2019-20 Kindergarten                       : chr [1:1459] "0" "24" "0" "16" ...
 $ 2019-20 Grade One                          : chr [1:1459] "0" "20" "0" "10" ...
 $ 2019-20 Grade Two                          : chr [1:1459] "0" "17" "0" "15" ...
 $ 2019-20 Grade Three                        : chr [1:1459] "0" "19" "0" "16" ...
 $ 2019-20 Grade Four                         : chr [1:1459] "1" "21" "0" "11" ...
 $ 2019-20 Grade Five                         : chr [1:1459] "0" "17" "0" "18" ...
 $ 2019-20 Grade Six                          : chr [1:1459] "1" "26" "0" "17" ...
 $ 2019-20 Grade Seven                        : chr [1:1459] "3" "29" "0" "21" ...
 $ 2019-20 Grade Eight                        : chr [1:1459] "2" "30" "0" "17" ...
 $ 2019-20 Grade Nine                         : chr [1:1459] "0" "0" "22" "27" ...
 $ 2019-20 Grade Ten                          : chr [1:1459] "0" "0" "27" "49" ...
 $ 2019-20 Grade Eleven                       : chr [1:1459] "0" "0" "20" "38" ...
 $ 2019-20 Grade Twelve                       : chr [1:1459] "0" "0" "20" "66" ...
> 
> ethnicities <- ode_schools %>% select(attnd_schl_inst_id = `Attending School ID`,
+                                       sch_name = `School Name`,
+                                       contains("%")) %>%
+   janitor::clean_names()
> 
> names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))
> 
> head(ethnicities)
# A tibble: 6 x 9
  attnd_schl_inst… sch_name p_american_indi… p_asian p_native_hawaii…
             <dbl> <chr>               <dbl>   <dbl>            <dbl>
1              498 Adel El…          0.286    0                     0
2              707 Adrian …          0        0                     0
3              708 Adrian …          0.0112   0.0112                0
4               17 Alsea C…          0.0156   0.0561                0
5             1208 Amity E…          0.0240   0                     0
6             1210 Amity H…          0.00794  0.0119                0
# … with 4 more variables: p_black_african_american <dbl>,
#   p_hispanic_latino <dbl>, p_white <dbl>, p_multiracial <dbl>
> 
> 
> #make ethnicities have attnd_schl_inst_id as a unique identifier - clunky, but works!
> ethnicities <- ethnicities %>%
+   group_by(attnd_schl_inst_id) %>%
+   summarize(p_american_indian_alaska_native = mean(p_american_indian_alaska_native),
+             p_asian = mean(p_asian),
+             p_native_hawaiian_pacific_islander = mean(p_native_hawaiian_pacific_islander),
+             p_black_african_american = mean(p_black_african_american),
+             p_hispanic_latino = mean(p_hispanic_latino),
+             p_white = mean(p_white),
+             p_multiracial = mean(p_multiracial))
`summarise()` ungrouping output (override with `.groups` argument)
> 
> 
> #Join ethnicity data and training data
> full_train <- left_join(full_train, ethnicities)
Joining, by = "attnd_schl_inst_id"
> 
> #dim(full_train)
> 
> # import tidied frl and student count data 
> frl <- read_csv("data/frl_stucounts.csv")

── Column specification ────────────────────────────────────────────────────────
cols(
  ncessch = col_double(),
  free_lunch_qualified = col_double(),
  reduced_price_lunch_qualified = col_double(),
  missing = col_double(),
  not_applicable = col_double(),
  no_category_codes = col_double(),
  n = col_double()
)

> 
> # add frl data to train data
> frl_fulltrain <- left_join(full_train, frl)
Joining, by = "ncessch"
> 
> 
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 0.1.2 ──
✔ broom     0.7.2      ✔ recipes   0.1.15
✔ dials     0.0.9      ✔ rsample   0.0.8 
✔ infer     0.5.3      ✔ tune      0.1.2 
✔ modeldata 0.1.0      ✔ workflows 0.2.1 
✔ parsnip   0.1.4      ✔ yardstick 0.0.7 
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ scales::discard() masks purrr::discard()
✖ dplyr::filter()   masks stats::filter()
✖ recipes::fixed()  masks stringr::fixed()
✖ dplyr::lag()      masks stats::lag()
✖ yardstick::spec() masks readr::spec()
✖ recipes::step()   masks stats::step()
> library(xgboost)

Attaching package: ‘xgboost’

The following object is masked from ‘package:dplyr’:

    slice

> 
> set.seed(500)
> edu_split <- initial_split(frl_fulltrain)
> train <- training(edu_split)
> test <- testing(edu_split)
> train_cv <- vfold_cv(train, strata = "score", v = 10)
> 
> #create recipe and prep
> rec <- recipe(score ~., train) %>%
+   step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
+               time_index = as.numeric(tst_dt)) %>%
+   step_rm(contains("bnch")) %>%
+   update_role(tst_dt, new_role = "time_index") %>%
+   update_role(contains("id"), ncessch, new_role = "id") %>% #removed sch_name
+   step_novel(all_nominal(), -all_outcomes()) %>%
+   step_unknown(all_nominal(), -all_outcomes()) %>%
+   step_rollimpute(all_numeric(), -all_outcomes(), -has_role("id"), -n) %>%
+   step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id")) %>%
+   step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
+   step_dummy(all_nominal(), -has_role(match = "id"), -all_outcomes(), -time_index) %>%
+   step_nzv(all_predictors()) %>%
+   step_interact(terms = ~ starts_with('lat'):starts_with("lon"))
> 
> prep(rec)
Data Recipe

Inputs:

       role #variables
         id          6
    outcome          1
  predictor         44
 time_index          1

Training data contained 142070 data points and 142070 incomplete rows. 

Operations:

Variable mutation for tst_dt, time_index [trained]
Variables removed tst_bnch [trained]
Novel factor level assignment for gndr, ethnic_cd, calc_admn_cd, ... [trained]
Unknown factor level assignment for gndr, ethnic_cd, calc_admn_cd, ... [trained]
Rolling Imputation for enrl_grd, lat, ... [trained]
Median Imputation for enrl_grd, lat, ... [trained]
Sparse, unbalanced variable filter removed calc_admn_cd, missing, not_applicable [trained]
Dummy variables from gndr, ethnic_cd, migrant_ed_fg, ind_ed_fg, ... [trained]
Sparse, unbalanced variable filter removed 85 items [trained]
Interactions with lat:lon [trained]
> 
> set.seed(500)
> 
> #default model without tuning
> mod <- boost_tree() %>% 
+   set_engine("xgboost", nthreads = parallel::detectCores()) %>% 
+   set_mode("regression") %>% 
+   set_args(trees = 5000, #number of trees in the ensemble
+            stop_iter = 20, #the number of iterations without improvement before stopping
+            validation = 0.2,
+            learn_rate = 0.1) #the rate at which boosting algoirithm adapts at each iteration
> 
> 
> #create workflow for default boosted model
> wf_df <- workflow() %>% 
+   add_recipe(rec) %>% 
+   add_model(mod)
> 
> 
> #let's get a sense of what this tuned model looks like
> translate(mod)
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 5000
  learn_rate = 0.1
  stop_iter = 20

Engine-Specific Arguments:
  nthreads = parallel::detectCores()
  validation = 0.2

Computational engine: xgboost 

Model fit template:
parsnip::xgb_train(x = missing_arg(), y = missing_arg(), nrounds = 5000, 
    eta = 0.1, early_stop = 20, nthreads = parallel::detectCores(), 
    validation = 0.2, nthread = 1, verbose = 0)
> 
> 
> #fit model w/ tuned learning rate
> tune_tree_lr <- fit_resamples(
+   wf_df, 
+   train_cv, 
+   metrics = metric_set(rmse, rsq),
+   control = control_resamples(verbose = TRUE,
+                               save_pred = TRUE,
+                               extract = function(x) extract_model(x)))

Attaching package: ‘rlang’

The following objects are masked from ‘package:purrr’:

    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,
    flatten_lgl, flatten_raw, invoke, list_along, modify, prepend,
    splice


Attaching package: ‘vctrs’

The following object is masked from ‘package:dplyr’:

    data_frame

The following object is masked from ‘package:tibble’:

    data_frame

i Fold01: preprocessor 1/1
✓ Fold01: preprocessor 1/1
i Fold01: preprocessor 1/1, model 1/1
✓ Fold01: preprocessor 1/1, model 1/1
i Fold01: preprocessor 1/1, model 1/1 (predictions)
i Fold02: preprocessor 1/1
✓ Fold02: preprocessor 1/1
i Fold02: preprocessor 1/1, model 1/1
✓ Fold02: preprocessor 1/1, model 1/1
i Fold02: preprocessor 1/1, model 1/1 (predictions)
i Fold03: preprocessor 1/1
✓ Fold03: preprocessor 1/1
i Fold03: preprocessor 1/1, model 1/1
✓ Fold03: preprocessor 1/1, model 1/1
i Fold03: preprocessor 1/1, model 1/1 (predictions)
i Fold04: preprocessor 1/1
✓ Fold04: preprocessor 1/1
i Fold04: preprocessor 1/1, model 1/1
✓ Fold04: preprocessor 1/1, model 1/1
i Fold04: preprocessor 1/1, model 1/1 (predictions)
i Fold05: preprocessor 1/1
✓ Fold05: preprocessor 1/1
i Fold05: preprocessor 1/1, model 1/1
✓ Fold05: preprocessor 1/1, model 1/1
i Fold05: preprocessor 1/1, model 1/1 (predictions)
i Fold06: preprocessor 1/1
✓ Fold06: preprocessor 1/1
i Fold06: preprocessor 1/1, model 1/1
✓ Fold06: preprocessor 1/1, model 1/1
i Fold06: preprocessor 1/1, model 1/1 (predictions)
i Fold07: preprocessor 1/1
✓ Fold07: preprocessor 1/1
i Fold07: preprocessor 1/1, model 1/1
✓ Fold07: preprocessor 1/1, model 1/1
i Fold07: preprocessor 1/1, model 1/1 (predictions)
i Fold08: preprocessor 1/1
✓ Fold08: preprocessor 1/1
i Fold08: preprocessor 1/1, model 1/1
✓ Fold08: preprocessor 1/1, model 1/1
i Fold08: preprocessor 1/1, model 1/1 (predictions)
i Fold09: preprocessor 1/1
✓ Fold09: preprocessor 1/1
i Fold09: preprocessor 1/1, model 1/1
✓ Fold09: preprocessor 1/1, model 1/1
i Fold09: preprocessor 1/1, model 1/1 (predictions)
i Fold10: preprocessor 1/1
✓ Fold10: preprocessor 1/1
i Fold10: preprocessor 1/1, model 1/1
✓ Fold10: preprocessor 1/1, model 1/1
i Fold10: preprocessor 1/1, model 1/1 (predictions)
> 
> saveRDS(tune_tree_lr, "BTTuneTalapasGrid1.Rds")
> 
> 
> #collect metrics
> bt_grid_met <- tune_tree_lr %>%
+   collect_metrics() 
> 
> bt_grid_met %>%
+   write.csv("./BTTuneMetricsDefault.csv", row.names = FALSE)
> 
> 
> 
> #apply to test split
> test_fit <- last_fit(wf_df, edu_split)
> test_metrics <- test_fit$.metrics
> 
> test_metrics %>%
+   write.csv("./BTTestMetrics1.csv", row.names = FALSE)
> 
> #make predictions on test.csv using this final workflow
> full_test <- read_csv("data/test.csv",
+                       col_types = cols(.default = col_guess(), 
+                                        calc_admn_cd = col_character()))
> #str(full_test)
> 
> #join with ethnicity data
> full_test_eth <- left_join(full_test, ethnicities) #join with FRL dataset
Joining, by = "attnd_schl_inst_id"
> #str(full_test_eth)
> 
> #join with frl
> full_test_FRL <- left_join(full_test_eth, frl)
Joining, by = "ncessch"
> 
> nrow(full_test_FRL)
[1] 63142
> 
> #workflow
> fit_workflow <- fit(wf_df, frl_fulltrain)
> 
> #use model to make predictions for test dataset
> preds_final <- predict(fit_workflow, full_test_FRL) #use model to make predictions for test dataset
> 
> saveRDS(preds_final, "BTPreds.Rds")
> 
> head(preds_final)
# A tibble: 6 x 1
  .pred
  <dbl>
1 2524.
2 2528.
3 2612.
4 2382.
5 2518.
6 2503.
> 
> pred_frame <- tibble(Id = full_test_FRL$id, Predicted = preds_final$.pred)
> head(pred_frame, 20)
# A tibble: 20 x 2
      Id Predicted
   <dbl>     <dbl>
 1     4     2524.
 2     6     2528.
 3     8     2612.
 4     9     2382.
 5    11     2518.
 6    15     2503.
 7    19     2390.
 8    20     2493.
 9    32     2375.
10    33     2423.
11    40     2608.
12    42     2437.
13    46     2493.
14    50     2540.
15    57     2524.
16    59     2490.
17    66     2597.
18    73     2540.
19    78     2456.
20    91     2392.
> nrow(pred_frame)
[1] 63142
> 
> #create prediction file
> write_csv(pred_frame, "fit_bt.csv")
> 
> proc.time()
    user   system  elapsed 
1889.476   23.552 1913.740 
